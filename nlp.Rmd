---
title: "nlp.Rmd"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(magrittr)
library(scales)
library(stringr)
library(tibble)
library(tidyr)
library(tidytext)


'%out%' <- Negate ('%in%')
load("n_enron.Rda")
data(stop_words)

exempt_stopwords <- enframe(c("against", "all", "allow", "allows", "always", "awfully", "beforehand", "behind", "below", "better", "big", "can't", "cannot", "cant", "case", "cases", "downwards", "except", "gives", "good", "great", "highest", "hopefully", "immediate", "might", "necessary", "not", "nowhere", "numbers", "otherwise", "point", "serious", "seriously", "state", "states", "unfortunately", "value", "worked", "working", "zero"))
colnames(exempt_stopwords) <- c("line", "word")
stop_words <- anti_join(stop_words, exempt_stopwords)
add_stops <- c("ect", "hou", "enronxgate")
lexicon <- c("user", "user", "user")
new_stops <- cbind(add_stops, lexicon)
colnames(new_stops) <- c("word", "lexicon")
new_stops <- as_tibble(new_stops)
stop_words <- rbind(stop_words, new_stops)

n_text  <- n_enron %>% select(payload)
n1_text <- n_enron %>% filter(f_cluster == 1) %>% select(payload)
n2_text <- n_enron %>% filter(f_cluster == 2) %>% select(payload)
n3_text <- n_enron %>% filter(f_cluster == 3) %>% select(payload)

n_words <- n_text                             %>%
  unnest_tokens(word, payload)                %>%
  anti_join(stop_words)                       %>%
  filter(word != str_match_all(word,"\\d+"))  %>%
  filter(word != str_match_all(word,"\\d+.\\d+"))

n1_words <- n1_text                           %>%
  unnest_tokens(word, payload)                %>%
  anti_join(stop_words)                       %>%
  filter(word != str_match_all(word,"\\d+"))  %>%
  filter(word != str_match_all(word,"\\d+.\\d+"))

n2_words <- n2_text                           %>%
  unnest_tokens(word, payload)                %>%
  anti_join(stop_words)                       %>%
  filter(word != str_match_all(word,"\\d+"))  %>%
  filter(word != str_match_all(word,"\\d+.\\d+"))

n3_words <- n3_text                           %>%
  unnest_tokens(word, payload)                %>%
  anti_join(stop_words)                       %>%
  filter(word != str_match_all(word,"\\d+"))  %>%
  filter(word != str_match_all(word,"\\d+.\\d+"))

n_dist  <- n_words  %>% distinct()
n1_dist <- n1_words %>% distinct()
n2_dist <- n2_words %>% distinct()
n3_dist <- n3_words %>% distinct()

n1_vocab_n2 <- setdiff(n1_dist, n2_dist)
n1_vocab    <- setdiff(n1_vocab_n2, n3_dist)
n2_vocab_n1 <- setdiff(n2_dist, n1_dist)
n2_vocab    <- setdiff(n2_vocab_n1, n3_dist)
n3_vocab    <- setdiff(n3_dist, union(n1_dist, n2_dist))
n_vocab     <- setdiff(n_dist, union(n1_vocab, n2_vocab, n3_vocab))

n_dist_bag  <- n_words  %>%
  mutate(Cluster0 = word %in% n_vocab$word)   %>%
  filter(Cluster0 == TRUE)

n1_dist_bag <- n1_words %>%
  mutate(Cluster1 = word %in% n1_vocab$word)  %>%
  filter(Cluster1 == TRUE)

n2_dist_bag <- n2_words %>%
  mutate(Cluster2 = word %in% n2_vocab$word)  %>%
  filter(Cluster2 == TRUE)

n3_dist_bag <- n3_words %>%
  mutate(Cluster3 = word %in% n3_vocab$word)  %>%
  filter(Cluster3 == TRUE)
```

The tibble extracted from the graph analysis contains two fields that will be used to determine word usage differences, if any, among the three graph clusters, payload (the original content of the email) and f_cluster (the cluster to which the email sender was assigned).

The dataset consists of `r nrow(n_enron)` records.

Any collection of natural language contains many high-frequency words that obscure differences. These words, such as *a, as, an, and, of, this, that, which, what* and the like are censored from analysis along with numerals.^[The entire original contents are retained in the original `n_enron` object, however.]

There are a variety of stopword lists. Some may be under-inclusive, removing only the most common parts of speech, while others may be over-inclusive, removing words that should be kept. For example, the word `not` often appears on lists of stopwords but is an important term in doing sentiment analysis using phrases -- *not good* has a difference valence than *[blank] good*.

The `stop_word` data from the `tidytext` package was hand edited to exempt the following list of words:

* against
* all
* allow
* allows
* always
* awfully
* beforehand
* behind
* below
* better
* big
* can't
* cannot
* cant
* case
* cases
* downwards
* except
* gives
* good
* great
* highest
* hopefully
* immediate
* might
* necessary
* not
* nowhere
* numbers
* otherwise
* point
* serious
* seriously
* state
* states
* unfortunately
* value
* worked
* working
* zero

and to add `ect`, `hou` and `enronxgate`, which from data cleansing done originally were found to be email address components from an older email system.

The `r nrow(n_enron)` mails contail a total of `r nrow(n_words)` words, of which `r nrow(n_dist)` are distinct.

As a whole, the following chart shows the most frequent words in `n_enron` that occur 150 or more times.

```{r, echo = FALSE}
n_words %>%
  count(word)                       %>%
  filter(n > 150)                   %>%
  mutate(word = reorder(word, n))   %>%
  arrange(desc(n))                  %>%
  ggplot(aes(word,n))                 	+
  geom_col()                          	+
  xlab("All Clusters Top Common Words")	+
  ylab("Occuring 150 times or more")   	+
  coord_flip()
```

Here are the top words in each cluster, after removing the adjusted stopwords and numbers.

```{r, echo = FALSE}
n1_words %>%
  count(word)                       %>%
  filter(n > 75)                    %>%
  mutate(word = reorder(word, n))   %>%
  arrange(desc(n))                  %>%
  ggplot(aes(word,n))               +
  geom_col()                        +
  xlab("All Cluster 1 Top Words")   +
  ylab("Occuring 75 times or more") +
  coord_flip()
n2_words %>%
  count(word, sort = TRUE)          %>%
  filter(n > 25)                    %>%
  mutate(word = reorder(word, n))   %>%
  ggplot(aes(word,n))               +
  geom_col()                        +
  xlab("All Cluster 2 Top Words")   +
  ylab("Occuring 25 times or more") +
  coord_flip()
n3_words %>%
  count(word, sort = TRUE)          %>%
  filter(n > 50)                    %>%
  mutate(word = reorder(word, n))   %>%
  ggplot(aes(word,n))               +
  geom_col()                        +
  xlab("All Cluster 3 Top Words")   +
  ylab("Occuring 50 times or more") +
  coord_flip()

```


The clusters differ in the number of total words: Cluster 1 has `r nrow(n1_words)`, Cluster 2, `r nrow(n2_words)` and Cluster 3, `r nrow(n3_words)`. 

The clusters also have distinct vocabularies. The three clusters all share `r nrow(n_vocab)` terms in common. Cluster 1 includes `r nrow(n1_vocab)` terms that do not appear in Clusters 2 or 3. Cluster 2 includes `r nrow(n2_vocab)` terms that do not appear in Clusters 1 or 3. Cluster 3 includes `r nrow(n3_vocab)` terms that do not appear in Clusters 1 or 2.

The top unique terms in each cluster are shown in the following three charts.

```{r, echo = FALSE}
n1_dist_bag                         %>%
  count(word)                       %>%
  filter(n > 25)                    %>%
  mutate(word = reorder(word, n))   %>%
  arrange(desc(n))                  %>%
  ggplot(aes(word,n))               +
  geom_col()                        +
  xlab("Cluster 1 Only Top Words")  +
  ylab("Occuring 25 times or more") +
  coord_flip()
n2_dist_bag                         %>%
  count(word)                       %>%
  filter(n > 5)                     %>%
  mutate(word = reorder(word, n))   %>%
  arrange(desc(n))                  %>%
  ggplot(aes(word,n))               +
  geom_col()                        +
  xlab("Cluster 2 Only Top Words")  +
  ylab("Occuring 5 times or more") +
  coord_flip()
n3_dist_bag                         %>%
  count(word)                       %>%
  filter(n > 10)                    %>%
  mutate(word = reorder(word, n))   %>%
  arrange(desc(n))                  %>%
  ggplot(aes(word,n))               +
  geom_col()                        +
  xlab("Cluster 3 Only Top Words")  +
  ylab("Occuring 10 times or more") +
  coord_flip()
```

The proportion of cluster-specific words to total words for each cluster are: 

* Cluster 1: `r nrow(n1_dist_bag)/nrow(n1_words)`
* Cluster 2: `r nrow(n2_dist_bag)/nrow(n2_words)`
* Cluster 3: `r nrow(n3_dist_bag)/nrow(n3_words)`

To test whether the differences in vocabulary and frequently used terms, a correlation^[Pearson product-moment correlation] test may be applied.

```{r, echo = FALSE}
n_clusters <- n_words %>%
  mutate(Cluster1 = word %in% n1_dist_bag$word) %>% 
  mutate(Cluster2 = word %in% n2_dist_bag$word) %>% 
  mutate(Cluster3 = word %in% n3_dist_bag$word)

n_freq <- n_clusters %>% 
  group_by(word,Cluster1,Cluster2,Cluster3) %>% 
  count(word)                               %>% 
  mutate(frequency = n/nrow(.))             %>%
  ungroup()

n_freq <- n_freq %>% 
  mutate(Cluster1 = Cluster1 * 1) %>%
  mutate(Cluster2 = Cluster2 * 1) %>%
  mutate(Cluster3 = Cluster3 * 1) 
  
n_freq <- n_freq %>%
  mutate(Cluster1 = Cluster1*frequency) %>%
  mutate(Cluster2 = Cluster2*frequency) %>%
  mutate(Cluster3 = Cluster3*frequency)
  

n_freq <- n_freq %>%
  mutate(Cluster1 = Cluster1*frequency) %>%
  mutate(Cluster2 = Cluster2*frequency) %>%
  mutate(Cluster3 = Cluster3*frequency)


cor.test(n_freq$Cluster1,n_freq$Cluster2)
cor.test(n_freq$Cluster1,n_freq$Cluster3)
cor.test(n_freq$Cluster2,n_freq$Cluster3)

```

With respect to word frequencies, the three clusters are very weakly negatively correlated. The three clusters can be distinguished not only by volume, percentage of cluster-specific vocabulary, but also by the lack of intra-cluster correlations. Because the clusters derive from the machine learning latent network detection algorithm, there is now credible evidence that they can be further analyzed to identify collections of specific messages containing keywords of interest, to which we next turn.
