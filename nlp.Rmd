---
title: "nlp.Rmd"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


```

## Natural language processing

### Exploratory data analysis

The tibble extracted from the graph analysis contains two fields that will be used to determine word usage differences, if any, among the three graph clusters, payload (the original content of the email) and f_cluster (the cluster to which the email sender was assigned).

The dataset consists of `r nrow(n_enron)` records.

Any collection of natural language contains many high-frequency words that obscure differences. These words, such as *a, as, an, and, of, this, that, which, what* and the like are censored from analysis along with numerals.^[The entire original contents are retained in the original `n_enron` object, however.]

There are a variety of stopword lists. Some may be under-inclusive, removing only the most common parts of speech, while others may be over-inclusive, removing words that should be kept. For example, the word `not` often appears on lists of stopwords but is an important term in doing sentiment analysis using phrases -- *not good* has a difference valence than *[blank] good*.

##################################### UPDATE
The `stop_word` data from the `tidytext` package to add `ect`, `hou` and `enronxgate`, which from data cleansing done originally were found to be email address components from an older email system.

The `r nrow(n_enron)` mails contail a total of `r nrow(n_words)` words, of which `r nrow(n_dist)` are distinct.

As a whole, the following chart shows the most frequent words in `n_enron` that occur 150 or more times.

```{r, echo = FALSE}
n_words %>%
  count(word)                       %>%
  filter(n > 150)                   %>%
  mutate(word = reorder(word, n))   %>%
  arrange(desc(n))                  %>%
  ggplot(aes(word,n))                 	+
  geom_col()                          	+
  xlab("All Clusters Top Common Words")	+
  ylab("Occuring 150 times or more")   	+
  coord_flip()
```

Here are the top words in each cluster, after removing the adjusted stopwords and numbers.

```{r, echo = FALSE}
n1_words %>%
  count(word)                       %>%
  filter(n > 75)                    %>%
  mutate(word = reorder(word, n))   %>%
  arrange(desc(n))                  %>%
  ggplot(aes(word,n))               +
  geom_col()                        +
  xlab("All Cluster 1 Top Words")   +
  ylab("Occuring 75 times or more") +
  coord_flip()
n2_words %>%
  count(word, sort = TRUE)          %>%
  filter(n > 25)                    %>%
  mutate(word = reorder(word, n))   %>%
  ggplot(aes(word,n))               +
  geom_col()                        +
  xlab("All Cluster 2 Top Words")   +
  ylab("Occuring 25 times or more") +
  coord_flip()
n3_words %>%
  count(word, sort = TRUE)          %>%
  filter(n > 50)                    %>%
  mutate(word = reorder(word, n))   %>%
  ggplot(aes(word,n))               +
  geom_col()                        +
  xlab("All Cluster 3 Top Words")   +
  ylab("Occuring 50 times or more") +
  coord_flip()

```


The clusters differ in the number of total words: Cluster 1 has `r nrow(n1_words)`, Cluster 2, `r nrow(n2_words)` and Cluster 3, `r nrow(n3_words)`. 

The clusters also have distinct vocabularies. The three clusters all share `r nrow(n_vocab)` terms in common. Cluster 1 includes `r nrow(n1_vocab)` terms that do not appear in Clusters 2 or 3. Cluster 2 includes `r nrow(n2_vocab)` terms that do not appear in Clusters 1 or 3. Cluster 3 includes `r nrow(n3_vocab)` terms that do not appear in Clusters 1 or 2.

The top unique terms in each cluster are shown in the following three charts.

```{r, echo = FALSE}
n1_dist_bag                         %>%
  count(word)                       %>%
  filter(n > 25)                    %>%
  mutate(word = reorder(word, n))   %>%
  arrange(desc(n))                  %>%
  ggplot(aes(word,n))               +
  geom_col()                        +
  xlab("Cluster 1 Only Top Words")  +
  ylab("Occuring 25 times or more") +
  coord_flip()
n2_dist_bag                         %>%
  count(word)                       %>%
  filter(n > 5)                     %>%
  mutate(word = reorder(word, n))   %>%
  arrange(desc(n))                  %>%
  ggplot(aes(word,n))               +
  geom_col()                        +
  xlab("Cluster 2 Only Top Words")  +
  ylab("Occuring 5 times or more") +
  coord_flip()
n3_dist_bag                         %>%
  count(word)                       %>%
  filter(n > 10)                    %>%
  mutate(word = reorder(word, n))   %>%
  arrange(desc(n))                  %>%
  ggplot(aes(word,n))               +
  geom_col()                        +
  xlab("Cluster 3 Only Top Words")  +
  ylab("Occuring 10 times or more") +
  coord_flip()
```

The proportion of cluster-specific words to total words for each cluster are: 

* Cluster 1: `r nrow(n1_dist_bag)/nrow(n1_words)`
* Cluster 2: `r nrow(n2_dist_bag)/nrow(n2_words)`
* Cluster 3: `r nrow(n3_dist_bag)/nrow(n3_words)`

To test whether the differences in vocabulary and frequently used terms, a correlation^[Pearson product-moment correlation] test may be applied.

```{r, echo = FALSE}
n_clusters <- n_words %>%
  mutate(Cluster1 = word %in% n1_dist_bag$word) %>% 
  mutate(Cluster2 = word %in% n2_dist_bag$word) %>% 
  mutate(Cluster3 = word %in% n3_dist_bag$word)

n_freq <- n_clusters %>% 
  group_by(word,Cluster1,Cluster2,Cluster3) %>% 
  count(word)                               %>% 
  mutate(frequency = n/nrow(.))             %>%
  ungroup()

n_freq <- n_freq %>% 
  mutate(Cluster1 = Cluster1 * 1) %>%
  mutate(Cluster2 = Cluster2 * 1) %>%
  mutate(Cluster3 = Cluster3 * 1) 
  
n_freq <- n_freq %>%
  mutate(Cluster1 = Cluster1*frequency) %>%
  mutate(Cluster2 = Cluster2*frequency) %>%
  mutate(Cluster3 = Cluster3*frequency)
  

n_freq <- n_freq %>%
  mutate(Cluster1 = Cluster1*frequency) %>%
  mutate(Cluster2 = Cluster2*frequency) %>%
  mutate(Cluster3 = Cluster3*frequency)


pander(cor.test(n_freq$Cluster1,n_freq$Cluster2))
pander(cor.test(n_freq$Cluster1,n_freq$Cluster3))
pander(cor.test(n_freq$Cluster2,n_freq$Cluster3))

```

With respect to word frequencies, the three clusters have very week negative coefficients, but the corresponding p-values require rejection of the null hypothosis that they are uncorrelated. The three clusters can be distinguished not only by volume, percentage of cluster-specific vocabulary, but also by the lack of intra-cluster correlations. 

However, term frequency alone overweights frequently occurring words and should be supplemented by considering rarely occurring words in combination.^[The analysis of `td-idf` here follows the examples given in chapter 3 of [Tidytext].] A plot of how often a term occurs within each cluster by its rank order shows the straight line log-log plot characteristic of a power-law distribution.

```{r}
freq_by_rank %>%
 ggplot(aes(rank, `term frequency`, color = f_cluster))   +
 geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE)  +
 scale_x_log10()                                          +
 scale_y_log10()
```

Regressing the log of term frequency on the log of term rank illustrates how closely the distribution follows that model.

```{r}
pander(summary(fit))
```

The `td-idf` (term frequency/inverse term frequency) word table provides a more nuanced and useful depiction of differences in word usage among the graph clusters.

```{r}
c_words                           %>%
  select(-total)                  %>%
  arrange(desc(tf_idf))           %>%
  mutate(word =
    factor(word, levels =
             rev(unique(word))))  %>%
  group_by(f_cluster)             %>%
  top_n(16)                       %>%
  ungroup                         %>%
  ggplot(aes(word, tf_idf, fill = f_cluster))       +
  geom_col(show.legend = FALSE)                     +
  labs(x = NULL, y = "tf-idf")                      +
  facet_wrap(~f_cluster, ncol = 2, scales = "free") +
  coord_flip()
```

In these charts, there is more specificity. Names are more prominent, both natural and business, in clusters 1 and 3; while cluster 2 also contains names, it has more abstract terms than the other clusters. "Media, puhca^[Public Utilities Holding Company Act]. employment, and speech" are examples. The abstract terms "buys, sells, trades" in cluster 1 are suggestive of an operation that engages in frequent transactions. Cluster 3 has "transwestern, questar and nng", which were all natural gas pipeline distributors.

Although the word associations differences among are suggestive of distinct aspects of Enron's business, machine learning has the potential to further organize the email texts to find other connections and distinctions through a process of latent Dirichet allocation, or `LDA`^[Not to be confused with linear discriminant analysis.]

### Machine learning



