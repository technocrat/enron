---
title: "Topic Extraction"
author: "Richard Careaga"
date: "March 9, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(broom)
library(GGally)
library(here)
library(pander)
library(pdftools)
library(slam)
library(tidyr)
library(tidytext)
library(tidyverse)
library(tm)
library(topicmodels)
load("n_enron.Rda")

# Mon Mar 11 21:55:19 2019 ------------------------------
# uri <- here("sources/wolak.pdf")
# engine <- "pdftools"
# rseader <- readPDF(engine)
# wolak_text <- reader(elem = list(uri = uri), language = "en", id = "id1")$content # discard metadata
# See supplemental script wolak_scrub.R for operations
# to clean wolak_text for topic analysis as a single document
# rather than as a collection of documents (one per page).
# saved as wolak_clean.Rda

load("wolak_clean.Rda")

wolak <- VectorSource(wolak_clean)
w_corpus <- VCorpus(wolak)
w_corpus <- tm_map(w_corpus, removeWords, stopwords("english"))
w_corpus <- tm_map(w_corpus, removeNumbers)
#save(w_corpus, file = "w_corpus.Rda")
w_dtm <- DocumentTermMatrix(w_corpus)
removeSparseTerms(w_dtm, 0.2)
#save(w_dtm, file = "w_dtm.Rda")
findFreqTerms(w_dtm, 5)
# ff ch 6 of tt
w_lda <- LDA(w_dtm, control = list(seed = 2203), k = 6)
w_topics <- tidy(w_lda, matrix = "beta")
w_top_terms <- w_topics %>%
  group_by(topic) 	  	%>%
  top_n(15, beta) 		  %>%
  ungroup() 			      %>%
  arrange(topic, -beta)


#=================================================================
beta_spread2_1 <- w_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread2_3 <- w_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic3 > .001) %>%
  mutate(log_ratio = log2(topic3 / topic1))

beta_spread2_4 <- w_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic4 > .001) %>%
  mutate(log_ratio = log2(topic4 / topic1))

beta_spread2_5 <- w_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic5 > .001) %>%
  mutate(log_ratio = log2(topic5 / topic1))

beta_spread2_6 <- w_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic6 > .001) %>%
  mutate(log_ratio = log2(topic6 / topic1))

beta_spread3_1 <- w_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic3 > .001) %>%
  mutate(log_ratio = log2(topic3 / topic1))

beta_spread4_1 <- w_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic4 > .001) %>%
  mutate(log_ratio = log2(topic4 / topic1))

beta_spread4_3 <- w_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic3 > .001 | topic4 > .001) %>%
  mutate(log_ratio = log2(topic4 / topic3))

beta_spread4_5 <- w_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic5 > .001 | topic4 > .001) %>%
  mutate(log_ratio = log2(topic4 / topic5))

beta_spread4_6 <- w_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic6 > .001 | topic4 > .001) %>%
  mutate(log_ratio = log2(topic6 / topic4))

beta_spread5_6 <- w_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic6 > .001 | topic6 > .001) %>%
  mutate(log_ratio = log2(topic6 / topic5))


# begin analysis of Enron corpus with a email original content

cluster0_raw <- n_enron$payload
#save(cluster0_raw, file = "cluster0_raw.Rda")

# preprocessing "textified" newlines, returns, tabs and Notes hypenation

pattern1  <- "\\\n"
pattern2  <- "\\\t"
pattern3  <- "\\\r"
pattern4  <- "="

milled <- str_replace_all(cluster0_raw, pattern1, " ")
milled <- str_replace_all(milled, pattern2, " ")
milled <- str_replace_all(milled, pattern3, " ")
milled <- str_replace_all(milled, pattern4, " ")

# join list into a single character string

cluster0_cleaned <- paste(milled, sep = '', collapse = '')
#save(cluster0_cleaned, file = "cluster0_cleaned.Rda")

# Bring the unclustered Enron plaintext into a `tm` package object
c0_corpus <- VectorSource(cluster0_cleaned)
c0_corpus <- VCorpus(c0_corpus)
c0_corpus <- tm_map(c0_corpus, stripWhitespace)
c0_corpus <- tm_map(c0_corpus, content_transformer(tolower))
c0_corpus <- tm_map(c0_corpus, removeWords, stopwords("english"))
c0_corpus <- tm_map(c0_corpus, removeNumbers)
c0_corpus <- tm_map(c0_corpus, removePunctuation)

#save(w_corpus, file = "c0_corpus.Rda")

# create a document term matrix and remove seldom occurring word pairs
c0_dtm <- DocumentTermMatrix(c0_corpus)
removeSparseTerms(c0_dtm, 0.2)
#save(c0_dtm, file = "c0_dtm.Rda")

# Inspect the frequent terms for the document

findFreqTerms(c0_dtm, 5)

# This code follows the example in the [Tidytext] book.

# Create a test model for four topics

c0_lda <- LDA(c0_dtm, control = list(seed = 2203), k = 4)

# extract topics and their respective per topic per word frequencies



c0_topics <- tidy(c0_lda, matrix = "beta")

c0_top_terms <- c0_topics %>%
  group_by(topic)    	  	%>%
  top_n(25, beta) 	    	%>%
  ungroup() 			        %>%
  arrange(topic, -beta)

c0_beta_spread2_1 <- c0_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

c0_beta_spread2_3 <- c0_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic3 > .001) %>%
  mutate(log_ratio = log2(topic3 / topic1))

c0_beta_spread2_4 <- c0_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic4 > .001) %>%
  mutate(log_ratio = log2(topic4 / topic1))

c0_beta_spread3_1 <- c0_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic3 > .001) %>%
  mutate(log_ratio = log2(topic3 / topic1))

c0_beta_spread4_1 <- c0_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic4 > .001) %>%
  mutate(log_ratio = log2(topic4 / topic1))

c0_beta_spread4_3 <- c0_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic3 > .001 | topic4 > .001) %>%
  mutate(log_ratio = log2(topic4 / topic3))

cluster1_raw <- n_enron %>% filter(f_cluster == 1) %>% select(payload)
save(cluster1_raw, file = "cluster1_raw.Rda")

# preprocessing "textified" newlines, returns, tabs and Notes hypenation

pattern1  <- "\\\\n"
pattern2  <- "\\\\t"
pattern3  <- "\\\\r"
pattern4  <- "="

milled <- str_replace_all(cluster1_raw, pattern1, " ")
milled <- str_replace_all(milled, pattern2, " ")
milled <- str_replace_all(milled, pattern3, " ")
milled <- str_replace_all(milled, pattern4, " ")

# join list into a single character string

cluster1_cleaned <- paste(milled, sep = '', collapse = '')
#save(cluster1_cleaned, file = "cluster1_cleaned.Rda")

# Bring the unclustered Enron plaintext into a `tm` package object
c1_corpus <- VectorSource(cluster1_cleaned)
c1_corpus <- VCorpus(c1_corpus)
c1_corpus <- tm_map(c1_corpus, stripWhitespace)
c1_corpus <- tm_map(c1_corpus, content_transformer(tolower))
c1_corpus <- tm_map(c1_corpus, removeWords, stopwords("english"))
c1_corpus <- tm_map(c1_corpus, removeNumbers)
c1_corpus <- tm_map(c1_corpus, removePunctuation)

#save(c1_corpus, file = "c1_corpus.Rda")

# create a document term matrix and remove seldom occurring word pairs
c1_dtm <- DocumentTermMatrix(c1_corpus)
# removeSparseTerms(c1_dtm, 1.2) c1 is not sparse
#save(c1_dtm, file = "c1_dtm.Rda")

# Inspect the frequent terms for the document

#findFreqTerms(c1_dtm, 5)

# This code follows the example in the [tidytext] book.

# Create a model for three topics

c1_lda <- LDA(c1_dtm, control = list(seed = 2213), k = 3)

# extract topics and their respective per topic per word frequencies

c1_topics <- tidy(c1_lda, matrix = "beta")

c1_top_terms <- c1_topics 	%>%
  group_by(topic)    	  	%>%
  top_n(25, beta) 	    	%>%
  ungroup() 			    %>%
  arrange(topic, -beta)

cluster2_raw <- n_enron %>% filter(f_cluster == 2) %>% select(payload)
#save(cluster2_raw, file = "cluster2_raw.Rda")

# preprocessing "textified" newlines, returns, tabs and Notes hypenation

pattern1  <- "\\\\n"
pattern2  <- "\\\\t"
pattern3  <- "\\\\r"
pattern4  <- "="

milled <- str_replace_all(cluster2_raw, pattern1, " ")
milled <- str_replace_all(milled, pattern2, " ")
milled <- str_replace_all(milled, pattern3, " ")
milled <- str_replace_all(milled, pattern4, " ")

# join list into a single character string

cluster2_cleaned <- paste(milled, sep = '', collapse = '')
#save(cluster2_cleaned, file = "cluster2_cleaned.Rda")

# Bring the unclustered Enron plaintext into a `tm` package object
c2_corpus <- VectorSource(cluster2_cleaned)
c2_corpus <- VCorpus(c2_corpus)
c2_corpus <- tm_map(c2_corpus, stripWhitespace)
c2_corpus <- tm_map(c2_corpus, content_transformer(tolower))
c2_corpus <- tm_map(c2_corpus, removeWords, stopwords("english"))
c2_corpus <- tm_map(c2_corpus, removeNumbers)
c2_corpus <- tm_map(c2_corpus, removePunctuation)

#save(c2_corpus, file = "c2_corpus.Rda")

# create a document term matrix and remove seldom occurring word pairs
c2_dtm <- DocumentTermMatrix(c2_corpus)
# removeSparseTerms(c2_dtm, 2.2) c2 is not sparse
#save(c2_dtm, file = "c2_dtm.Rda")

# Inspect the frequent terms for the document

#findFreqTerms(c2_dtm, 5)

# This code follows the example in the [tidytext] book.

# Create a model for three topics

c2_lda <- LDA(c2_dtm, control = list(seed = 2223), k = 3)

# extract topics and their respective per topic per word frequencies

c2_topics <- tidy(c2_lda, matrix = "beta")

c2_top_terms <- c2_topics 	%>%
  group_by(topic)    	  	%>%
  top_n(25, beta) 	    	%>%
  ungroup() 			    %>%
  arrange(topic, -beta)

cluster3_raw <- n_enron %>% filter(f_cluster == 3) %>% select(payload)
#save(cluster3_raw, file = "cluster3_raw.Rda")

# preprocessing "textified" newlines, returns, tabs and Notes hypenation

pattern1  <- "\\\\n"
pattern3  <- "\\\\t"
pattern3  <- "\\\\r"
pattern4  <- "="

milled <- str_replace_all(cluster3_raw, pattern1, " ")
milled <- str_replace_all(milled, pattern2, " ")
milled <- str_replace_all(milled, pattern3, " ")
milled <- str_replace_all(milled, pattern4, " ")

# join list into a single character string

cluster3_cleaned <- paste(milled, sep = '', collapse = '')
#save(cluster3_cleaned, file = "cluster3_cleaned.Rda")

# Bring the unclustered Enron plaintext into a `tm` package object
c3_corpus <- VectorSource(cluster3_cleaned)
c3_corpus <- VCorpus(c3_corpus)
c3_corpus <- tm_map(c3_corpus, stripWhitespace)
c3_corpus <- tm_map(c3_corpus, content_transformer(tolower))
c3_corpus <- tm_map(c3_corpus, removeWords, stopwords("english"))
c3_corpus <- tm_map(c3_corpus, removeNumbers)
c3_corpus <- tm_map(c3_corpus, removePunctuation)

#save(c3_corpus, file = "c3_corpus.Rda")

# create a document term matrix and remove seldom occurring word pairs
c3_dtm <- DocumentTermMatrix(c3_corpus)
#removeSparseTerms(c3_dtm, 3.3) c3 is not sparse
#save(c3_dtm, file = "c3_dtm.Rda")

# Inspect the frequent terms for the document

#findFreqTerms(c3_dtm, 5)

# This code follows the example in the [tidytext] book.

# Create a model for three topics

c3_lda <- LDA(c3_dtm, control = list(seed = 33), k = 3)

# extract topics and their respective per topic per word frequencies

c3_topics <- tidy(c3_lda, matrix = "beta")

c3_top_terms <- c3_topics 	%>%
  group_by(topic)    	  	%>%
  top_n(35, beta) 	    	%>%
  ungroup() 			    %>%
  arrange(topic, -beta)

c1_beta_spread2_1 <- c1_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

c1_beta_spread2_3 <- c1_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic3 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

c1_beta_spread3_1 <- c1_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic3 > .001) %>%
  mutate(log_ratio = log2(topic3 / topic1))

```

### Email discovery in litigation

Rule 34 of [The Federal Rules of Civil Procedure] permits parties to civil litigation to require opposing parties and, in some cases, third parties, to produce evidence or materials likely to lead to the production of evidence that is stored in electronic form. Regulatory and law enforcement agencies can also require or subpoena electronic information, including email. Given the adverse interests, time and cost constraints, and the sheer volume of electronic records, including email, both sides struggle with making the process manageable. The civil courts require parties to engage in a cooperative process (see the [Sedona Conference] Report, the report of the [Advisory Committee] on the most recent changes to Rule 34 and the [ESI Guidelines] and [ESI Checklist] issued by the U.S. District Court for the Northern District of California).

Among the questions of greatest important is how to separate the relevant content from the large universe of what is available. For that, participants usually turn to search terms -- keywords and phrases. (*See* recent article pointing out [keyword limitations] on effective discovery.)

As the author of the [keyword limitations] article points out, words have synonyms, private meanings, multiple meanings and meanings that depend critically on context. Less appreciated, perhaps, is that most people fail to distinguish between styles of language. While it's easy to tell the difference between an inaugural address and a sports color commentator's styles in oral communication and to distinguish a scientific paper from a young adult novel in formal written conversation, email falls in the  separate sphere of informal written language.^[*See* [McWhorter].]

> Reading email is eavesdropping. (*Author*)

### Information sought

Prior to the investigation that resulted in the an initial staff report, FERC received a report from the California agency responsible for operating the electric energy exchange that FERC was concerned that the market had been manipulating.^[Frank A. Wolak, Chairman, Market Surveillance Committee of California Independent System Operator, *Report on Redesign of California Real-Time Energy and Ancillary Services Markets* dated October 18, 1999], which will be referred to as the [Wolak report].

The [Wolak report] was prepared before FERC began its investigation, did not rely on the Enron email corpus^[The report does not even name Enron.] and provides a lengthy analysis of the indicia of the exercise of market power in the California electricity wholesale market. A topical and keyword digest was prepared, as discussed below, to identify subjects of interest in the corpus.

#### Wolak keywords and topics

A latent Dirichlet allocation model specifying classification into six topics was prepared for the [Wolak Report]^[A *topic* does not necessarily correspond to how an author has organized a document.]. As described in the [classic paper] by David Blei, Andrew Ng and Michael Jordan, *Latent Dirichlet Allocation* (*J. Machine Learning Res.* 3 (2003). 993-1022), this machine learning technique is a generative probabilistic model. Each discrete object in a collection (such as words in a document) is "modeled as a finite mixture over an underlying set of topics" (*id.*). Topics on the other hand are "modeled as an infinite mixture over an underlying set of topic probabilities." (*id.*) A list of some of the keywords in each modeled topic are

```{r, echo=FALSE}
pander(w_top_terms)
w_top_terms 							%>%
  mutate(term = reorder(term, beta)) 	%>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) 				+
  facet_wrap(~ topic, scales = "free") 			+
  coord_flip()
```

What is immediately striking is that the same word, such as *market* may belong to every topic. Other words, *PG&E, hydro,* and *divestiture* are found only in one topic.

Looking at pairwise differences between each topic in terms of the ratio of most used vs. least used terms highlights differences.

```{r, echo=FALSE}

beta_spread2_3 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 2 / topic 3") +
  coord_flip()

beta_spread2_4 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 2 / topic 4") +
  coord_flip()

beta_spread2_5 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 2 / topic 5") +
  coord_flip()

beta_spread2_6 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 2 / topic 6") +
  coord_flip()

beta_spread3_1 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 3 / topic 1") +
  coord_flip()

beta_spread4_1 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 4 / topic 1") +
  coord_flip()

beta_spread4_3 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 4 / topic 3") +
  coord_flip()

beta_spread4_5 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 4 / topic 5") +
  coord_flip()

beta_spread4_6 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 4 / topic 6") +
  coord_flip()

beta_spread5_6 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 5 / topic 6") +
  coord_flip()

```

* Topic 1 is a theoretical analysis of market power in the California energy market.

* Topic 2 focuses on distortions in the market caused by the retail rate freeze that was instituted together with creation of generation divestiture by California electric utilities and the creation of the Independent System Operator (ISO or CAISO) to schedule day-ahead prices for electric power.

* Topic 3 is a also a theoretical analysis of market power in the California energy market.

* Topic 4 discusses proposed changes by the ISO designed to relieve interzonal^[California has northern and southern zones, *ND* and *SD*.] transmission congestion by imposing operating conditions on new generation systems to connect to the power grid.

* Topic 5 discussed proposed changes by the ISO designed to relieve interzonal transmission congestion by imposing reqiuirements that new generating facilities mitigate any adverse effects on transmission systems and overall reliability of supply delivery.

* Topic 6 deals with Pacific Gas & Electric's (PG&E) hydroelectric assets.

Topics 1 and 3 are the most relevant to FERC's investigation and its collection of the Enron email corpus.

A combination of keywords and topics will be derived from the Enron dataset and commonalities will be sought.

### Enron topics as a whole

A similar topical analysis of the `n_enron` dataset yields a strikingly different picture of the top 25 words in each topic.

```{r, echo=FALSE}
c0_top_terms                          %>%
  mutate(term = reorder(term, beta)) 	%>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE)          				+
  facet_wrap(~ topic, scales = "free") 	    		+
  coord_flip()
```

They are nearly all monosylabbic, names of people^[Including a stray Notes email name *arnoldhouectect*], many are conversational in tone -- *please, thank you, need, today* and quotidian terms relating to meetings and the like.

```{r, echo=FALSE}
c0_beta_spread2_3 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 2 / topic 3") +
  coord_flip()

c0_beta_spread2_4 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 2 / topic 4") +
  coord_flip()

c0_beta_spread3_1 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 3 / topic 1") +
  coord_flip()

c0_beta_spread4_1 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 4 / topic 1") +
  coord_flip()

c0_beta_spread4_3 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 4 / topic 3") +
  coord_flip()

```

These results stand in marked contrast to the [Wolak] report in terms of vocabulary, word frequency and latent topics, and even the vocabulary difference among latent network clusters discussed earlier, posing the question *Why?*

*Why*, asked Alan Jay Lerner, in his adaptation of Bernard Shaw's *Pygmalion,* *can't a woman be more like a man?* The question, posed in 1956, based on a 1912 play offends modern sensibilities, as well it should. However, an exactly parallel question, without the gender baggage exists: *Why can't a businessperson compose emails like an academic paper or a legal complaint?*

The two principal reasons are first, email is much more like speech than formal comoposition; second, every group of specialists has a vocabulary to describe what they do, and that vocabulary bears no necessary relationship to how outsiders describe the same conduct.

It is unlikely, for example that the following passage will be found anywhere in the Enron corpus

> Through collusion with our strategic partners who own generating assets in California, we will be able to exercise anti-competitive market power to reap excessive prices in the hour-ahead market by manipulating bidding in the day-ahead market.

### Enron topics by social network cluster

Each of the three graph clusters was modeled into three topics, with the following results.


```{r, echo=FALSE}
c1_top_terms                          %>%
  mutate(term = reorder(term, beta)) 	%>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE)          				+
  facet_wrap(~ topic, scales = "free") 	    		+
  coord_flip()
c2_top_terms                          %>%
  mutate(term = reorder(term, beta)) 	%>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE)          				+
  facet_wrap(~ topic, scales = "free") 	    		+
  coord_flip()
c3_top_terms                          %>%
  mutate(term = reorder(term, beta)) 	%>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE)          				+
  facet_wrap(~ topic, scales = "free") 	    		+
  coord_flip()
```

Some hints can be gleaned from topic modeling the three clusters.

In the first cluster, *gas* and related terms is prominent in the first topic, but missing from the second and much more prominent in the third. The first has more tranactional terms -- *deal, book, trade* than the third. The second cluster has only *deal* as a transactional term. A plausible hypothosis is that cluster one is engaged in running a trading operation, cluster two is primarily long-range in orientation and cluster three is focused on operating the gas distribution business.

Athough the foregoing observations are speculative, an avenue of further explanation is to examine the log ratio differences in cluster 1, which appears to be the most transactional, to see if different types of transactional activity can be detected.

```{r, echo=FALSE}
c1_beta_spread3_1 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 3 / topic 1") +
  coord_flip()

c1_beta_spread2_3 %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Cluster 1 log2 ratio of beta in topic 2 / topic 3") +
  coord_flip()
```

