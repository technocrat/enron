---
title: "Outline of eDiscovery paper"
author: "Richard Careaga"
date: "February 8, 2019"

output: 
    html_document

# header-includes:
#    - \usepackage{fancyvrb}
#    - \usepackage{hyperref}
#    - \usepackage{graphicx}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
                      
suppressPackageStartupMessages(library(network))
suppressPackageStartupMessages(library(rebus))
suppressPackageStartupMessages(library(statnet))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tm))
#suppressPackageStartupMessages(library(UserNetR))
library(network)
library(tidyverse)
#library(UserNetR)
#library(rebus)
library(sna)
library(statnet)
library(tm)
load("g_enron.Rda")

```


# Introduction

## Goal

The goal of this paper is to illustrate a combination of machine learning, natural language processing and graph analysis techniques applied to corporate email to identify potential witnesses in litigation.

# Background

> In times of political turmoil, events can move from impossible to inevitable without even passing through improbable. [Anatole Kalesky]

[Enron Corp.] and its affiliates were engaged in energy-related businesses, as described in its [Annual Report on Form 10-K for the year ended December 31, 2000]

\begin{Verbatim}[frame=single]

*    the transportation of natural gas through pipelines to
markets throughout the United States;

*    the generation, transmission and distribution of
electricity to markets in the northwestern United States;

*    the marketing of natural gas, electricity and other
commodities and related risk management and finance services
worldwide;

*    the development, construction and operation of power
plants, pipelines and other energy related assets worldwide;

*    the delivery and management of energy commodities and
capabilities to end-use retail customers in the industrial
and commercial business sectors; and

*    the development of an intelligent network platform to
provide bandwidth management services and the delivery of
high bandwidth communication applications.

As of December 31, 2000, Enron employed approximately
20,600 persons.

	\end{Verbatim}


For the year ended December 31, 2000, it had operating revenues of $100,789 million, according to the same report.

On December 2, 2001, Enron filed for [bankruptcy protection].

In less than a year, Enron underwent a complete reversal of fortune as its business strategies ran afoul of applicable regulations, among which were those of the Federal Energy Regulatory Commission (**FERC**).

FERC [became aware] of irregularities in the California wholesale electricity market prices. An orientation to the issues is provided by [testimony] before FERC, which  provides a concise summary.^[The short version, which I can relate as a former California regulatory official from personal knowledge, is that public electric utilities were losing a large share of industrial customers to self-generation. Many businesses found it cheaper to generate on-site than to pay tariff rates. Foreseeably, residential and business customers without the option to self-generate would come to  bear the entire cost of unamortized utility fixed assets^[Termed *stranded costs*], and rates for retail, commercial and small industrial customers would increase. The adopted solution was to require the utilities to sell their generation plants and buy power on a new public market on a *day-ahead*, tomorrow's estimated demand, and an *hour-ahead* basis. Although much thought was devoted to the dangers that participants would game the system to sell or buy at discounts from market, insufficient consideration was given to mufti-participant cooperation.]

Following Enron's bankruptcy, FERC began an intense investigation, including the email records of 149 Enron employees. A preliminary [staff report] issued six months later.

## Motivating Data

FERC obtained approximately 500,000 emails. Copies of these were acquired by Leslie Kaelbling of MIT and [published] by William W. Cohen of Carnegie Mellon University. It is one of the largest publicly available datasets of corporate email and is referred to as the Enron Corpus. The term *corpus* is used in natural language processing to denote a collection of related text.

At the time, electronic record examination (*ediscovery*) in litigation was in a primitive state. It was not uncommon, for example, for paper copies of email to be offered. These would typically be read by teams of freelance attorneys looking for keywords. Advanced technology included scanning with optical character recognition and some proprietary software options to organize emails and capture the status of review. 
Much of the focus was directed to keyword searches, sometimes called the *smoking gun* approach. Brute force examination misses opportunities to understand the social networks that reflect how the organization operates, what their concerns are and the haphazard exposure of document reviewers inevitably poses the [Elephant and the Blind Men Problem]. To triage the corpus quickly and efficiently, it should first be  be distilled and analyzed.

# Analysis

## Data acquisition

I obtained a copy of the [2009 version] of the corpus. It contains copies of emails of a private nature that involve three users have who since requested to be [redacted]. I have removed those 27 emails.^[Most of my work on data wrangling and preliminary took place in 2018 in Python, relying heavily on the NLTK and networkx packages.]

### Conversion

Each email was a plaintext file^[Most had been generated by Microsoft Outlook, but some older emails were produced in IBM Notes, which created some character encoding issues.] Each user had a directory tree similar to the one below.^[This user had 10 directories with 3048 files (the directory tree has been pruned to omit spurious detail) containing 12,147 lines and 69,226 words.]

![Typical user data tree](https://s3-us-west-2.amazonaws.com/dslabs.nostromo/dtree.jpg)

Although tedious, traversing the directory tree, parsing the emails and loading them into an SQL database, was accomplished with a combination of Python and Perl scripting and standard bash tools. I do not reproduce that process here as it has little bearing on the main topic of this paper.

### Data structure

While the emails were not in native format, the plain text versions contained nine principal segments, as shown in the figure below

![Structural analysis of an Enron email](https://s3-us-west-2.amazonaws.com/tuva/parse_email.png)

Of those, the following were extracted from my earlier analysis for this paper:

* sender
* date
* subject
* recipient(s)
* message body
* new content

### Deduplication

Using scripting tools, each text file extraction created a *payload* of the new content in the related email, capturing the text between the beginning metadata and the following metadata for email purposes. A `payload` hash, an [md5] encoded message digest^[In theory, it is possible that two non-identical sequences of bytes be encoded identically; the probability is low enough to make an md5 digest usable as a checksum verification, its purpose here.] was used in the initial analysis as a primary key to assure the uniqueness of each record. Approximately half of the corpus consisted of duplicates, such as the original message in the sender's sent file and one or more copies in the recipient's inbox, at a minimum. Multiple recipients and recipients who used email folders as a filing system were another source of duplicate messages. Applying this filter reduced the corpus to approximately 250,000 emails.

### Text isolation

For natural language processing (**NLP**) purposes, treating the `payload` rather than the `message body` as the unit of analysis avoided an *echo chamber* effect of `chains` quoting and re-quoting the original message, multiplying the frequency of the words it contained.

### Prioritization

Traditional analysis of emails was conducted on the principe that *something may be overlooked,* which delays the value of email in preliminary analysis. Prioritizing always leaves open the option of reviewing the set-asides later.

After deduplication, the first filter applied was to eliminate all email from external addresses that were not also recipients from internal addresses. Spam, newsletters and the like have low information potential. This filter reduced the remaining half of the corpus by half again, leaving approximately 125,000 emails.

A second filter for internal email was used to eliminate broadcast messages and high frequency administrative messages. Indicia of broadcast messages were large numbers of recipients, high frequency, paucity of return correspondence and keyword in context screening. Administrative messages to single recipients were identified by frequency, lack of return correspondence and high frequency words. Many of these were nagging emails concerning the lack of approval of expense reports, for example. This filter reduced the dataset to approximately 24,000.

The third filter limited the dataset to emails sent before Enron's December 2, 2001 bankruptcy. This filter reduced the email count to approximately 13,500, about 2.7% of the original total.^[*Ninety percent of everything is crap.* Theodore Sturgeon's Revelation, made in a dominantly paper-based information environment. *See also* Pareto distributions.] A few emails dated "1979-12-31" were reviewed and deleted.^[In the technology of the day, user desktop computers relied on an internal clock powered by a battery; when the battery died, the date reset to what the operating system, usually a variant of Windows, considered as the beginning of time.]

### Augmentation and transformation

Each unique Enron address in the reduced dataset was assigned a userid. The primary purpose was to facilitate social network analysis with node identifiers of uniform length; the second, to reduce analyst bias arising from gender stereotyping, frequency of exposure and similar subjective pattern seeking behaviors. 

The next transformation was to create an additional field with a `corpus` object to facilitate natural language processing.

    enron <- enron %>% mutate(edge_corp = map(payload, tm::VectorSource)

To achieve a computationally practicable dataset for initial social network analysis, emails were limited to single Enron sender to Enron single recipient, reducing the dataset further, to 9,615 emails. The resulting graph had low density, and the the dataset was further trimmed by restricting it to reciprocal correspondents, each of whom sent an email to the other, either as a reply or an original message, excluding emails by a user to herself. This further reduced the dataset, to 6,009 emails among 465 users. Finally, many emails were found to be blank or extremely short -- fewer than 10 characters, resulting in a final count of 5,922 emails among 445 users.

## Social network analysis

### The nature of social networks

Following the reduction of the corpus, the remaining senders and receivers were natural persons who engaged in mutual correspondence. These constitute `nodes` or `verticess` and their emails `edges`^[Or `arcs`, when directionality is considered]. Draw theee points and connect them, and you have created three nodes and three edges, a triagle, which is termed a `graph` object. A graph object encapsulates many useful features aside from who knows whom^[Such as the parlor game [six degrees of Kevin Bacon]] including measures of density, centrality, connectedness, separation, clustering and other indicia of how well or poorly embedded in an organization any individual may stand.

A graph object has the added advantage that its `edges` (connectors) can be loaded with an arbitrary number of attributes, such as date and semantic analysis content. For example a graph may be partitioned by topics -- some correspondents may have a purely non-business social relationship, others may be concerned solely with facilities maintenance, and yet others may have a strong association with trading commodities.

Graphs are potentially computationally intensive, which motivated the reduction of the selection of emails and users to approximately 1.4% of the emails available for examination.^[Together with the observation of the many sources of noise arising from the high volume of emails not relevant to any of the inquiries being made, including, for example, inspirational quotes of the day from an external site.]

Graphs are not only a processing unit, they constitute the domain of their own branch of mathematics.^[*See, e.g.*, the brief tutorial by [Keijo Ruohonen]].

### Description of the Enron graph

#### Visualization

```{r p_raw, echo=FALSE}
netmat_raw <- g_enron %>% select(t_userid, f_userid)
net_raw <- network(netmat_raw, matrix.type = "edgelist")
net_p_raw <- plot(net_raw, vertex.cex = 0.1)
```

The graph shows a dense cluster of users with a high concentration of potentially connected gropus of mutually interacting pairs surrounded by a cloud of low interactivity.

#### Size

```{r net_raw}
net_raw
```

The graph consists of `r length(net_raw$mel)` edges, each representing one or more emails, an average of `r length(net_raw$mel)/net_raw$gal$n` emails per user.

#### Density

The graph has a density of `r gden(net_raw)`, which is very low.

#### Components

```{r connectedness, echo=FALSE, results='hide'}
cd_weak    <- component.dist(net_raw ,connected = "weak")
net_raw_weak   <- length(cd_weak$csize)
cd_strong  <- component.dist(net_raw ,connected = "strong")
net_raw_strong <- length(cd_strong$csize)
```

As seen in the visualization of the graph, two components are present, one that is strongly connected, and the other, weakly, comprising `r net_raw_strong` and `r net_raw_weak` members, respectively.

#### Diameter

```{r diameter, echo=FALSE, results='hide'}
lgc <- component.largest(net_raw, result = "graph")
gd <- geodist(lgc)
```

The diameter of the graph, `r max(gd$gdist)`, is the number of steps required to connect the two nodes furthest apart.

#### Clustering

```{r clustering, echo=FALSE, results='hide', warning=FALSE}
clus <- gtrans(net_raw, mode = "graph", use.adjacency = FALSE)
```

The clustering coefficient of the graph is low, `r clus`, indicating relatively few users who belong to interacting groups.

### Description of the reduced Enron graph

```{r extract, echo=FALSE, results='hide'}
net <- delete.vertices(net_raw, isolates(net_raw))
net_vertices <- net$gal$n
net
```

Eliminating isolates reduces the number of connection endpoints from `r net_vertices` to `r net_vertices`. Within the original graph, the number of isolated node pairs, those that do not connect to the dense cluster in the center, can be eliminated to show the most interactive exchanges, thusly

    net <- delete.vertices(net_raw, isolates(net_raw))


#### Visualization

The contrast between the original and reduced graph (after removing isolates) is visually striking. Note that the line *length* does not any meaning; they vary solely to promote better visual discrimination.

```{r p, echo=FALSE}

gplot(net, usearrows = FALSE, edge.col = "gray80", lwd = 0.5)

```

The reduced graph is the dense portion of the original, but some more readily discernable structure is apparent. 

#### Density

The graph has a density of `r gden(net)`,  compared to the original graph, `r gden(net_raw)` or `r gden(net)/gden(net_raw)` greater.

#### Components

```{r connectedness_1, echo=FALSE, results='hide'}
cd_weak    <- component.dist(net ,connected = "weak")
net_weak   <- length(cd_weak$csize)
cd_strong  <- component.dist(net ,connected = "strong")
net_strong <- length(cd_strong$csize)
```

As seen in the visualization of the graph, only `r net_weak` member is weakly connected.

#### Diameter

```{r diameter_1, echo=FALSE, results='hide'}
lgc <- component.largest(net, result = "graph")
gd <- geodist(lgc)
```

The diameter of the graph, `r max(gd$gdist)`, the number of steps required to connect the two nodes furthest apart, is unchanged from the original.

#### Clustering

```{r clustering_1, echo=FALSE, results='hide', warning=FALSE}
clus <- gtrans(net, mode = "graph", use.adjacency = FALSE)
```

The clustering coefficient of the graph is low, `r clus`, essentially identical to the original graph, indicating relatively few users who belong to interacting groups.

### Decomposition of the reduced Enron graph

Among the nodes represented in the visualization above, some have satellite nodes that connect principally to these center points. As a reminder, the length of the lines carries no meaning, nor the distance or proximity between dense portions of the graph. The rendering algorithm's principal purpose is to spread out the links to promote visual clarity.

Although, visual inspection shows some *remote* nodes, every node is directly or indirectly connected to every other node in the graph, the number of isoletes is `r isolates(net)`. However, visual inspection also shows that *some* nodes are much more connected than others.

To identify these, measures of **centrality**. Three of those are `degree`, which measures aspects of the prominence of a node (user) in the network based on how many or few intermediates are required to reach any other node. The metric `betweenedness` representes how many connections between other nodes must pass through it to indirectly connect to nodes to which they are not directly connected. The third measure `infocent` measures the directness of a node to other nodes. Other measures of centrality exist, but the scope of this paper excludes a comparison among them.

Instead, the three were measured for each node in the reduced graph, giving them equal weight. The top 10% of nodes with high `degree` scores were selected, followed among those by the top 10% of high `betweenedness` scores, then high `infocent` scores to create a composite centrality indicator. Subsequent adjustments may improve the efficacy of the machine learning approach in this paper.

```{r centrality}
load("prominence.Rda")
prominence %>% select(deg, btw, inf) %>% cor()
prominence
```

The three measures of prominence are correlated, to varying degrees, indicating that they are measuring the characteristic of centrality non-independently. Just `r nrow(prominence)` nodes are in the top 10% of centrality. 

```{r prominence}
load("centrals.Rda")
net_p <- network(prominence, matrix.type = "edgelist")
vertices <- network.vertex.names(net_p)
prominent <- g_enron %>% filter(f_userid %in% centrals | t_userid %in% centrals)
```

These `r length(net_p$mel)` users are connected to `r length(vertices) - length(net_p$mel)` other users. They account for `r nrow(prominent)/nrow(g_enron)*100`% of the emails, as sender or receiver, in the reduced graph.









## Natural language processing

### Information sought

Prior to the investigation that resulted in the [staff] report, FERC received a report from the California agency responsible for operating the electric energy exchange that FERC was concerned that Enron had been manipulating.^[Frank A. Wolak, Chairman, Market Surveillance Committee of California Independent System Operator, *Report on Redesign of California Real-Time Energy and Ancillary Services Markets* dated October 18, 1999], which will be referred to as the [Wolak report].

The [Wolak report] was prepared before FERC began its investigation, did not rely on the Enron email corpus^[The report does not even name Enron.] and provides a lengthy analysis of the indicia of the exercise of market power in the California electricity wholesale market. A topical and keyword digest was prepared, as discussed below, to identify subjects of interest in the corpus.

# digest Wolak here

## Identification of high-value witnesses

# Results

### Resulting dataset

For purposes of this paper, the resulting tibble has been serialized to an `Rda` [file](https://s3-us-westâ€“2.amazonaws.com/dslabs.nostromo/g_enron.Rda) of approximately [ ] size, which can be brought into an `R` session with the following 

      # syntax to load Rda from S3
      con <- url("https://s3-us-west-2.amazonaws.com/dslabs.nostromo/g_enron.Rda")
      load(con)
      close(con)

# Conclusion

# Credits

[staff report]:http://elibrary.ferc.gov/idmws/common/opennat.asp?fileID=9548231

[Wolak report]:http://www.caiso.com/Documents/ReportonRedesign-CaliforniaReal-TimeEnergyandAncillaryServicesMarkets.pdf

[Anatole Kalesky]:https://www.project-syndicate.org/commentary/canceling-brexit-becoming-inevitable-by-anatole-kaletsky-2018-12

[Enron Corp.]:https://en.wikipedia.org/wiki/Enron

[Annual Report on Form 10-K for the year ended December 31, 2000]:https://www.sec.gov/Archives/edgar/data/1024401/000102440101500010/ene10-k.txt

[bankruptcy protection]:(https://www.sec.gov/Archives/edgar/data/1024401/000102440101500046/ene8-k1214.txt

[became aware]:https://www.ferc.gov/industries/electric/indus-act/wec/chron/chronology.pdf

[testimony]:https://web.stanford.edu/group/fwolak/cgi-bin/sites/default/files/files/2002,%20May%2015_Senate%20Committee%20on%20Commerce,%20Science%20and%20Transportation_Wolak.pdf

[Elephant and the Blind Men Problem]: https://buddhismnow.com/2018/02/16/tittha-sutta-buddhist-parable-of-the-blind-men-and-the-elephant/

[md5]:https://en.wikipedia.org/wiki/MD5

[published]:https://www.cs.cmu.edu/~./enron/

[redacted]:https://www.cs.cmu.edu/~./enron/DELETIONS.txt

[2009 version]:(https://www.cs.cmu.edu/~./enron/)

[six degrees of Kevin Bacon]:https://oracleofbacon.org/

[Keijo Ruohonen]:http://math.tut.fi/~ruohonen/GT_English.pdf

