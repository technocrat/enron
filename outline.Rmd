---
title: "Outline of eDiscovery paper"
author: "Richard Careaga"
date: "February 8, 2019"

output: 
    html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
suppressPackageStartupMessages(library(latentnet))
suppressPackageStartupMessages(library(network))
#suppressPackageStartupMessages(library(rebus))
suppressPackageStartupMessages(library(statnet))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(tm))
#suppressPackageStartupMessages(library(UserNetR))
library(coda)
library(tidyverse)
library(GGally)
#library(rebus)
library(tm)
library(tidyr)
library(tidytext)
'%out%' <- Negate ('%in%')
load("n_enron.Rda")
data(stop_words)

```

`## Natural language processing

### The Enron vocabulary

```{r}
n1_text <- n_enron %>% filter(f_cluster == 1) %>% select(payload)
n2_text <- n_enron %>% filter(f_cluster == 2) %>% select(payload)
n3_text <- n_enron %>% filter(f_cluster == 3) %>% select(payload)
n1_words <- n1_text %>% unnest_tokens(word, payload)
n2_words <- n2_text %>% unnest_tokens(word, payload)
n3_words <- n3_text %>% unnest_tokens(word, payload)
```

Cluster 1 has the most words, `r nrow(n1_words)`, followed by Cluster 3 with `r nrow((n3_words))` and Cluster 2 with `r nrow(n2_words)`.

Superficially, there is little difference among the clusters with respect to their ten most frequent words.

```{r}
n1_words %>% count(word, sort = TRUE)
n2_words %>% count(word, sort = TRUE)
n3_words %>% count(word, sort = TRUE)
```

With only two exceptions, the top words are among the most common in English. The two exceptions are `ect` in Cluster 1 and `kim` in Cluster 2. From the original data scrub `ect` is part of the internal email address system in use in the earlier emails and there may be a high number of persons named `kim` or `kim` is referred to often.

Looking at the 100 most frequent words, the pattern is similar. There are some additional first names, the term `enronxgate`. the word `gas` and the number `713`. The first word is also an older email component and `gas` was one of Enron's busineses. The number is simple the area code for Houston, where Enron's headquarters were located.

Some filtering is needed. This starts with subtracting a list of the most common words, such as *the* (**stopwords**). There are a variety of stopword lists. Some may be under-inclusive, removing only the most common parts of speech, while others may be over-inclusive, removing words that should be kept. For example, the word `not` often appears on lists of stopwords but is an important term in doing sentiment analysis using phrases -- *not good* has a difference valence that *[blank] good*.

The `stop_word` data from the `tidytext` package was hand edited to exempt the following list of works:

```{r}
exempt_stopwords <- enframe(c("against", "all", "allow", "allows", "always", "awfully", "beforehand", "behind", "below", "better", "big", "can't", "cannot", "cant", "case", "cases", "downwards", "except", "gives", "good", "great", "highest", "hopefully", "immediate", "might", "necessary", "not", "nowhere", "numbers", "otherwise", "point", "serious", "seriously", "state", "states", "unfortunately", "value", "worked", "working", "zero"))
colnames(exempt_stopwords) <- c("line", "word")
stop_words <- anti_join(stop_words, exempt_stopwords)
```


* against
* all
* allow
* allows
* always
* awfully
* beforehand
* behind
* below
* better
* big
* can't
* cannot
* cant
* case
* cases
* downwards
* except
* gives
* good
* great
* highest
* hopefully
* immediate
* might
* necessary
* not
* nowhere
* numbers
* otherwise
* point
* serious
* seriously
* state
* states
* unfortunately
* value
* worked
* working
* zero

and to add `ect`, `hou` and `enronxgate`.

```{r}
                       
add_stops <- c("ect", "hou", "enronxgate")
lexicon <- c("user", "user", "user")
new_stops <- cbind(add_stops, lexicon)
colnames(new_stops) <- c("word", "lexicon")
new_stops <- as_tibble(new_stops)
stop_words <- rbind(stop_words, new_stops)
```


Here are the top 25 words in each cluster, after removing the adjusted stopwords and numbers.


```{r}
n1_text <- n_enron %>% filter(f_cluster == 1) %>% select(payload)
n2_text <- n_enron %>% filter(f_cluster == 2) %>% select(payload)
n3_text <- n_enron %>% filter(f_cluster == 3) %>% select(payload)

n1_words <- n1_text %>%
            unnest_tokens(word, payload) %>% 
            anti_join(stop_words) %>% 
            filter(word != str_match_all(word,"^\\d+"))

n2_words <- n2_text %>%
            unnest_tokens(word, payload) %>% 
            anti_join(stop_words) %>% 
            filter(word != str_match_all(word,"^\\d+"))


n3_words <- n3_text %>%
            unnest_tokens(word, payload) %>% 
            anti_join(stop_words) %>% 
            filter(word != str_match_all(word,"^\\d+"))


n1_words %>% count(word, sort = TRUE) %>% print(n = 25)
n2_words %>% count(word, sort = TRUE) %>% print(n = 25)
n3_words %>% count(word, sort = TRUE) %>% print(n = 25)
```

A cursory glance at the lists for the three clusters show differences in the sizes of vocabulary: Cluster 1 has `r nrow(n1_words)`, Cluster 2, `r nrow(n2_words)` and Cluster 3, `r nrow(n3_words)`. Cluster 2 mentioned fewer names. Cluster 3 does not contain the word gas, and only Cluster 1 has the word energy.

A more rigorous assessment looks at each list in its entirety and calculates the correlation^[Pearon product-moment correlation] between each pair of lists.

```{r}
cor.test(data = frequency(n1_words), ~ proportion + n2_words)
```

### Information sought

Prior to the investigation that resulted in the an initial staff report, FERC received a report from the California agency responsible for operating the electric energy exchange that FERC was concerned that the market had been manipulating.^[Frank A. Wolak, Chairman, Market Surveillance Committee of California Independent System Operator, *Report on Redesign of California Real-Time Energy and Ancillary Services Markets* dated October 18, 1999], which will be referred to as the [Wolak report].

The [Wolak report] was prepared before FERC began its investigation, did not rely on the Enron email corpus^[The report does not even name Enron.] and provides a lengthy analysis of the indicia of the exercise of market power in the California electricity wholesale market. A topical and keyword digest was prepared, as discussed below, to identify subjects of interest in the corpus.

# digest Wolak here

## Identification of high-value witnesses

# Results

### Resulting dataset

For purposes of this paper, the resulting tibble has been serialized to an `Rda` [file](https://s3-us-westâ€“2.amazonaws.com/dslabs.nostromo/g_enron.Rda) of approximately [ ] size, which can be brought into an `R` session with the following 

      # syntax to load Rda from S3
      con <- url("https://s3-us-west-2.amazonaws.com/dslabs.nostromo/g_enron.Rda")
      load(con)
      close(con)

# Conclusion

# Credits

