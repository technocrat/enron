[
["introduction.html", "2 Introduction 2.1 Goal 2.2 Motivating Data 2.3 Applications", " 2 Introduction 2.1 Goal The goal of this paper is to illustrate a combination of machine learning, natural language processing and graph analysis techniques applied to corporate email to identify potential witnessess in litigation. 2.2 Motivating Data Enron Corporation was a $100 billion annual revenue company: They were in the gas and electricity business, mainly as traders, rather than as a utility California had an auction process for electricity that Enron was manipulating Enron used special purpose entities in a way that hid its financial condition There was a special purpose entity used for a deal involving barges in Nigeria Several individuals, including the CEO and his deputy were prosecuted Many employees lost their retirement savings when Enron stock became worthless The Federal Energy Regulatory Commission (FERC) investigated Enron’s activities in the western U.S. wholesale electricity market for evidence of price manipulation and other violations. It obtained approximately 500,000 copies of emails from 149 email users. Copies of these were acquired by Leslie Kaelbling of MIT and published by William W. Cohen of Carnegie Mellon University. It is one of the largest publicly available datasets of corporate email and is referred to as the Enron Corpus. The term corpus is used in natural language processing to denote a collection of related text. Civil and criminal litigation of other cases is conducted either by commercial or proprietary software. Much of the focus is directed to keyword searches and depends on visual scanning of emails by attorneys. Email examination can be a substantial expense. Although “smoking gun” emails may be found, brute force examination misses opportunities to understand the social networks that reflect how the organization operates, what their concerns are and which part of the corpus should receive priority. To do that the corpus must be distilled. ## Factual Background 2.3 Applications "],
["analysis.html", "3 Analysis 3.1 Data acquisition 3.2 Natural language processing 3.3 Social network analysis 3.4 Identification of high-value witnesses", " 3 Analysis 3.1 Data acquisition 3.1.1 Conversion 3.1.2 Deduplication 3.1.3 Metadata capture 3.1.4 Text isolation 3.1.5 Prioritization 3.1.6 Resulting dataset 3.2 Natural language processing 3.3 Social network analysis 3.4 Identification of high-value witnesses "],
["results.html", "4 Results", " 4 Results "],
["conclusion.html", "5 Conclusion", " 5 Conclusion "],
["credits.html", "6 Credits", " 6 Credits "]
]
