---
title: "Social Network Analysis of the Core Enron Corpus"
author: Richard Careaga
date:   2019-03-29
output: pdf_document
urlcolor: blue
---

```{r setup, echo=FALSE, warning=FALSE, output="asis", message=FALSE}

# silently load libraries

suppressPackageStartupMessages(library(ggnetwork))
suppressPackageStartupMessages(library(here))
suppressPackageStartupMessages(library(GGally))
suppressPackageStartupMessages(library(ggnetwork))
suppressPackageStartupMessages(library(ggthemes))
suppressPackageStartupMessages(library(hrbrthemes))
suppressPackageStartupMessages(library(kableExtra))
suppressPackageStartupMessages(library(latentnet))
suppressPackageStartupMessages(library(network))
suppressPackageStartupMessages(library(sna))
suppressPackageStartupMessages(library(statnet))
suppressPackageStartupMessages(library(statnet.common))
suppressPackageStartupMessages(library(tidyverse))

knitr::opts_chunk$set(echo = TRUE)

# make reproducible

set.seed(2203)

# functions

# verb to exclude items from another list

'%out%' <- Negate ('%in%')

# format digits to zero places

comma0 <- function(x) format(x, digits = 0, big.mark = ",")

# format digits to two places

comma2 <- function(x) format(x, digits = 2, big.mark = ",")

# create graph object 

netr <- function(x) {
	net <- network(x, matrix.type = "edgelist")
}

# remove isolates: remove disconnected users 

neti <- function(x) {
  delete.vertices(x, isolates(x))
}

# symmetrize: limit to two-way users

nets <- function(x) {
  x <- symmetrize(x , rule = "strong", return.as.edgelist=TRUE)
  x <- network(x, matrix.type = "edgelist")
}

# number of vertices in a network object

census <- function(x) {
  length(network.vertex.names(x))
}

# vertices names in a network object

cohort <- function(x) {
  network.vertex.names(x)
}

# plot with all vertix labels

node_plot <- function(x){
  ggnet2(x, label = TRUE, label.size = 2, node.color = "white",     edge.size = 0.25, edge.color = "grey")
}

# x is a network object, y is the date interval string

plot_graph <- function(x,y) {
ggplot(x, aes(x, y, xend = xend, yend = yend)) +
geom_edges(size = 0.25, color = "gray")               +
geom_nodes(size = 0.75, color = "steelblue")          +    
  labs(title="Graph of core Enron corpus",
       subtitle= y,		  
       caption="Source: Richard Careaga")			  + 
theme_ipsum_rc()									  +
theme_void()
}

# prepare tibble for time series plot 

# x is Qxxx_raw
Q_ts <- function(x) {
	x %>% group_by(date) %>% count()
}

# create a time series plot

# x is Qxxx_ts, a tibble object, y is the date interval character
plot_ts <- function(x,y) {
ggplot(data = x, aes(x = date, y = log10(n)))      				      + 
  geom_point(size = 0.75) + geom_smooth()                       +
  xlab("Month")                                                 +
  ylab("Number (log10) of private messages")                    +
  scale_x_date(date_breaks = "1 month", date_labels = "%y-%m")  +
  labs(title="Time series chart of core Enron corpus",
   	subtitle= y,		  
   	caption="Source: Richard Careaga")			  				+ 
   	theme(axis.text.x = element_text(angle = 30, hjust = 1))
}

# find the users with the top n betweenness scores 
# in a graph object	x
betw <-  function(x,n) {
	enframe(degree(x))        						%>% 
	mutate(userid = userid$userid[name]) 	%>%
	select(userid, value)			        		%>%
	rename(betweenness = value) 				  %>%
	arrange(desc(betweenness))					  %>%
	top_n(n)
}

# find the users with the top n closeness scores 
# in a graph object	x
clos <-  function(x,n) {
	enframe(closeness(x, cmode="suminvdir")) %>% 
	mutate(userid = userid$userid[name]) 	%>%
	select(userid, value)			        		%>%
	rename(closeness = value) 				    %>%
	arrange(desc(closeness))					    %>%
	top_n(n)
}

# find the users with the top n degree scores 
# in a graph object	x
deg <-  function(x,n) {
	enframe(degree(x))        						%>% 
	mutate(userid = userid$userid[name]) 	%>%
	select(userid, value)			        		%>%
	rename(degree = value)				      	%>%
	arrange(desc(degree))					        %>%
	top_n(n)
}

# Get x-y coordinates for selective plotting of userid
# x is a network object

get_xy <- function(x) {
  ggnetwork(x)
}


# data cleansing of recipient emails

# email recipient cleanser patterns 
faux_nl   	<-	"\\\n"
left_brak	  <-	"\\["
right_brak	<-	"\\]"

# ============================================================
# Code in this block commented out for `knitr` purposes
# load source data set from AWS

# con <- url("https://s3-us-west-2.amazonaws.com/dslabs.nostromo/enron.Rda")
# load(con)
# close(con)
# 
# # Note: tibbles do not serialize
# 
# enron <- as_tibble(enron)
# 
# # remove supurious characters
# 
# g_enron <- enron 								                		  %>% 
# 	mutate(tos = str_replace_all(tos, faux_nl, ""))		  %>%
# 	mutate(tos = str_replace_all(tos, left_brak, "")) 	%>%
# 	mutate(tos = str_replace_all(tos, right_brak, ""))	%>%
# 	mutate(ccs = str_replace_all(ccs, faux_nl, ""))		  %>%
# 	mutate(ccs = str_replace_all(ccs, left_brak, ""))	  %>%
# 	mutate(ccs = str_replace_all(ccs, right_brak, ""))	
# 
# # restrict to single recipient
# 
# g_enron <- g_enron %>% filter(tosctn == 1 & ccsctn == 0)
# 
# # censor non-enron addresses
# 
# g_enron <- g_enron                        %>% 
#   filter(str_detect(tos, "enron.com"))    %>%
#   filter(str_detect(sender, "enron.com")) %>%
#   select(sender, tos, date, lastword)     %>%
#   rename(recipient = tos)
# 
# # remove @enron.com from user names and single quotes
# 
# g_enron  <- g_enron                                           %>%
#   mutate(sender = str_replace_all(sender, ".enron.com", ""))  %>% 
#   mutate(recipient = str_replace_all(recipient, 
#                                      ".enron.com.", ""))      %>%
#   mutate(recipient = str_replace_all(recipient, "'", ""))
# 
# # censor internal group addresses
# 
# excluded_users <- c("all-hou.dl-bus", "all.america", "all.employees", "center.dl_portland", "chick.home", "clickathome", "clickathomepilot3", "dl-ga-all-ews", "dl-ga-all_enron_worldwide ", "dl-ga-all_enron_worldwide1", "dl-ga_all_enron_worldwide2", "dl-gal-all_enron_north_america2", "enron.chairman", "enron.gss", "executive.committee", "expense.report", "group.dl-ets", "helpdesk.mailman", "info", "outlook.team", "perfmgmt", "perfmgmt@ect", "portland", "portland.desk", "portland.shift", "sap_security", "the.globalist", "traders.eol", "trading .williams", "transportation.parking", "undisclosed-recipients")
# 
# g_enron <- g_enron %>% filter(sender %out% excluded_users)
# g_enron <- g_enron %>% filter(recipient %out% excluded_users)
# 
# # add record identification
# 
# g_enron <- rownames_to_column(g_enron, var = "recno")
# 
# # collect unique user names
# 
# sender    <- g_enron %>% select(sender)     %>% distinct()
# recipient <- g_enron %>% select(recipient)  %>% distinct()
# colnames(sender)    <- "user"
# colnames(recipient) <- "user"
# users <- bind_rows(sender,recipient)
# users <- users %>% distinct()
# 
# # assign numeric identifiers to users
# 
# set.seed(2203)
# user_pool <- seq(1000,nrow(users)+1001,1)
# userid <- enframe(sample(user_pool, nrow(users), 
#           replace = FALSE))                     %>%
#           select(-name)                         %>%
#           rename(userid = value)                %>%
#           mutate(userid = as.integer(userid))
# userid <- bind_cols(users, userid)
# 
# #save(userid, file = "userid.Rda")
# 
# # rename userid columns to join to g_enron as sender s_uid
# 
# colnames(userid) <- c("sender", "s_uid")
# g_enron <- left_join(g_enron, userid)
# 
# # again for recipient r_uid
# 
# colnames(userid) <- c("recipient", "r_uid")
# g_enron <- left_join(g_enron, userid)
# 
# # replace r_uid with userid
# 
# userid <- userid %>% rename(userid = r_uid)
# 
# # for inter-session conveience
#save(g_enron, file = "g_enron.Rda")

# # remove unneed objects from namespace
# 
# rm(enron, faux_nl, left_brak, right_brak, user_pool, users)

# divide into quarters and create file for all quarters
# including only symmetrical pairs sender -> recipient
# and recipient-sender messages in same period
# Naming convention: Q300 is Quarter 1 of 2000

# For convenience to avoid running commented block above

load("g_enron.Rda")
load("userid.Rda")  # reload originally assigned userids

# For the entire 23-month period and each quarter create objects

# tibble for time series

Qall_raw <- g_enron 									%>% 
	filter(date > "1999-12-31")					%>% 
	select(date, s_uid, r_uid)

# network object including all vertices

Qall_sparse <- Qall_raw 							%>% 
	select(-date) 										  %>% 
	netr(.) 

# network object eliminating isolates and non-reciprocal users

Qall <- Qall_sparse 									%>% 
	nets(.) 											      %>% 
	neti(.)

# Get x-y coordinates for selective plotting of userid
# x is a network object

get_xy <- function(x) {
  ggnetwork(x)
}

  
# Userids remaining in network

all_users <- network.vertex.names(Qall)

# create time series chart object
Qall_ts <- Q_ts(Qall_raw)

#Q399 3 vertices, omitted
#Q499 3 vertices, omitted

# same process for quarters; naming convention
# Q100 = quarter 1 of 2000

Q100_raw <- g_enron 									                %>% 
	filter (date >= "2000-12-31" & date < "2001-04-01") %>% 
	select(date, s_uid, r_uid)

Q100_sparse <- Q100_raw 							%>% 
	select(s_uid, r_uid) 								%>% 
	netr(.)

Q100 <- Q100_sparse 									%>% 
	nets(.) 											      %>% 
	neti(.)

Q100_ts <- Q_ts(Q100_raw)

Q200_raw <- g_enron 									                %>% 
	filter (date >= "2000-04-01" & date < "2000-07-01") %>% 
	select(date, s_uid, r_uid)

Q200_sparse <- Q200_raw 							%>%
	select(s_uid, r_uid) 								%>%
	netr(.) 

Q200 <- Q200_sparse 									%>% 
	nets(.) 											      %>% 
	neti(.)

Q200_ts <- Q_ts(Q200_raw)

Q300_raw <- g_enron 									                %>% 
	filter (date >= "2000-07-01" & date < "2000-10-01") %>% 
	select(date, s_uid, r_uid)

Q300_sparse <- Q300_raw 								%>%
	select(s_uid, r_uid)								  %>%
	netr(.)

Q300 <- Q300_sparse 									  %>% 
	nets(.) 											        %>%
	neti(.)

Q300_ts <- Q_ts(Q300_raw)

Q400_raw <- g_enron 									                %>% 
	filter (date >= "2000-10-01" & date < "2001-01-01") %>% 
	select(date, s_uid, r_uid)

Q400_sparse <- Q400_raw 								%>% 
	select(s_uid, r_uid) 								  %>%
	netr(.)

Q400 <- Q400_sparse										  %>% 
	nets(.)												        %>% 
	neti(.)

Q400_ts <- Q_ts(Q400_raw)

Q101_raw <- g_enron 									                %>% 
	filter (date >= "2001-01-01" & date < "2001-04-01") %>% 
	select(date, s_uid, r_uid)

Q101_sparse <- Q101_raw 								%>%
	select(s_uid, r_uid) 								  %>%
	netr(.) 

Q101 <- Q101_sparse										  %>%
	nets(.) 											        %>%
	neti(.)

Q101_ts <- Q_ts(Q101_raw)

Q201_raw <- g_enron 									                %>% 
	filter (date >= "2001-04-01" & date < "2001-07-01") %>% 
	select(date, s_uid, r_uid)

Q201_sparse <- Q201_raw 								%>% 
	select(s_uid, r_uid) 								  %>%
	netr(.)

Q201 <- Q201_sparse 									  %>% 
	nets(.) 											        %>%
	neti(.)

Q201_ts <- Q_ts(Q201_raw)

Q301_raw <- g_enron 									                %>% 
	filter (date >= "2001-07-01" & date < "2001-10-01") %>% 
	select(date, s_uid, r_uid)

Q301_sparse <- Q301_raw 								%>% 
	select(s_uid, r_uid) 								  %>% 
	netr(.)

Q301 <- Q301_sparse 										%>% 
	nets(.) 											        %>%
	neti(.)

Q301_ts <- Q_ts(Q301_raw)

Q401_raw <- g_enron										                %>% 
	filter (date >= "2001-10-01" & date < "2002-01-01") %>%
	select(date, s_uid, r_uid)

Q401_sparse <- Q401_raw 								%>% 
	select(s_uid, r_uid) 								  %>%
	netr(.) 

Q401 <- Q401_sparse 									  %>%
	nets(.) 											        %>% 
	neti(.)

Q401_ts <- Q_ts(Q401_raw)

# Quarterly census of users

Q100_c <- cohort(Q100)
Q200_c <- cohort(Q200)
Q300_c <- cohort(Q300)
Q400_c <- cohort(Q400)
Q101_c <- cohort(Q101)
Q201_c <- cohort(Q201)
Q301_c <- cohort(Q301)
Q401_c <- cohort(Q401)

# Users in all quarters

All_qtrs <- Reduce(intersect,  list(Q100_c,Q200_c,Q300_c,Q400_c,Q101_c,Q201_c,Q301_c,Q401_c))

# Email pairs in all quarters

msg_count <- g_enron                                  %>% 
  filter(s_uid %in% all_users & r_uid %in% all_users) %>%     
  group_by(s_uid, r_uid)                              %>% 
  count()                                             %>%
  ungroup()                                           %>%
  arrange(desc(n))

# Email senders in all quarters

s_uid_count <- g_enron                                %>% 
  filter(s_uid %in% all_users)                        %>%     
  group_by(s_uid)                                     %>% 
  count()                                             %>%
  ungroup()                                           %>%
  arrange(desc(n))

send_q1 <- annotate("text", x = 0.5, y = 75, label = "Single digits")
send_q2 <- annotate("text", x = 1.5, y = 75, label = "Double  digits")
send_q3 <- annotate("text", x = 2.5, y = 75, label = "Triple digits")

send_p <- ggplot(s_uid_count, aes(log10(n)))		+ 
	geom_bar() 										                + 
	coord_flip() 									                +
	xlab("Emails sent. log10") 						        +
	ylab("Number of users")		 					          +
	send_q1    										                +
	send_q2     	 								                +
	send_q3 	     								                +
	labs(title = "Number of emails sent per user",
        subtitle = "January 2000-November 2001",		  
        caption = "Source: Richard Careaga")		#+ 
	#theme_ipsum_rc()				

# Email recipients in all quarters

r_uid_count <- g_enron                                %>% 
  filter(r_uid %in% all_users)                        %>%     
  group_by(r_uid)                                     %>% 
  count()                                             %>%
  ungroup()                                           %>%
  arrange(desc(n))

recd_p <- ggplot(r_uid_count, aes(log10(n)))		+ 
	geom_bar() 										+ 
	coord_flip() 									+
	xlab("Emails (log10) received") 				+
	ylab("Number of users")		 					+
	send_q1    										+
	send_q2     	 								+
	send_q3 	     								+
	labs(title = "Emails received per user",
        subtitle = "January 2000-November 2001",		  
        caption = "Source: Richard Careaga")		#+ 
    #theme_ipsum_rc()
```


# Introduction

## Goal

The goal of this paper is to illustrate a combination of latent network detection and latent topic modeling machine learning, applied to corporate email to test whether the two classify email users similarly.

# Background

> In times of political turmoil, events can move from impossible to inevitable without even passing through improbable. [Anatole Kalesky]

[Enron Corp.] and its affiliates were engaged in energy-related businesses, as described in its [Annual Report on Form 10-K for the year ended December 31, 2000].

    *    the transportation of natural gas through pipelines to
    markets throughout the United States;
    
    *    the generation, transmission and distribution of
    electricity to markets in the northwestern United States;
    
    *    the marketing of natural gas, electricity and other
    commodities and related risk management and finance services
    worldwide;
    
    *    the development, construction and operation of power
    plants, pipelines and other energy related assets worldwide;
    
    *    the delivery and management of energy commodities and
    capabilities to end-use retail customers in the industrial
    and commercial business sectors; and
    
    *    the development of an intelligent network platform to
    provide bandwidth management services and the delivery of
    high bandwidth communication applications.

As of December 31, 2000, Enron employed approximately
20,600 persons.

For the year ended December 31, 2000, it had operating revenues of $100,789 million, according to the same report, in which it described one of its businesses as

> Enron purchases, markets and delivers natural gas, electricity and other commodities in North America. Customers include independent oil and gas producers, energy- intensive industrials, public and investor-owned utility power companies, small independent power producers and local distribution companies. Enron also offers a broad range of price, risk management and financing services including forward contracts, swap agreements and other contractual commitments. Enron's strategy is to enhance the scale, scope, flexibility and speed of its North American energy businesses through developing and acquiring selective assets, securing contractual access to third party assets, forming alliances with customers and utilizing technology such as EnronOnline.  With increased liquidity in the marketplace and the success of EnronOnline, Enron believes that it no longer needs to own the same level of physical assets, instead utilizing contracting and market-making activities.

On December 2, 2001, Enron filed for [bankruptcy protection].

In less than a year, Enron underwent a complete reversal of fortune as its business strategies ran afoul of applicable regulations, among which were those of the Federal Energy Regulatory Commission (**FERC**).

FERC [became aware] of irregularities in the California wholesale electricity market prices, a business in which Enron participated. An orientation to the issues is provided by [testimony] before FERC, which  provides a concise summary.^[The short version, which I can relate as a former California electric utility regulatory official from personal knowledge, is that public electric utilities were losing a large share of industrial customers to self-generation. Many businesses found it cheaper to generate on-site than to pay tariff rates. Foreseeably, residential and business customers without the option to self-generate would come to  bear the entire cost of unamortized utility fixed assets (termed *stranded costs*), and rates for retail, commercial and small industrial customers would increase. The adopted solution was to require the utilities to sell their generation plants and buy power on a new public market on a *day-ahead*, tomorrow's estimated demand, and an *hour-ahead* basis for unanticipated demand. Although much thought was devoted to the dangers that participants would game the system to sell or buy at discounts from market, insufficient consideration was given to multi-participant cooperation.]

Following Enron's bankruptcy, FERC began an intense investigation, including the email records of 149 Enron employees. A preliminary [staff report] issued six months later.

## Motivating Data

FERC obtained approximately 500,000 emails. Copies of these were acquired by Leslie Kaelbling of MIT and [published] by William W. Cohen of Carnegie Mellon University. It is one of the largest publicly available datasets of corporate email and is referred to as the Enron Corpus. The term *corpus* is used in natural language processing to denote a collection of related text.

At the time, electronic record examination (*ediscovery*) in litigation was in a primitive state. It was not uncommon, for example, for paper copies of email to be offered. These would typically be read by teams of freelance attorneys looking for keywords. Advanced technology included scanning with optical character recognition and some proprietary software options to organize emails and capture the status of review. 

Much of the focus was directed to keyword searches, sometimes called the *smoking gun* approach. Brute force examination misses opportunities to understand the social networks that reflect how the organization operates, what their concerns are and the haphazard exposure of document reviewers inevitably poses the [Elephant and the Blind Men Problem]. To triage the corpus quickly and efficiently, it should first be distilled and analyzed in terms of its network characteristics -- who corresponds privately with whom.

# Analysis

## Data acquisition

I obtained a copy of the [2009 version] of the corpus. It contains copies of emails of a private nature that involve three users have who since requested to be [redacted]. I have removed those 27 emails.^[Most of my work on data wrangling and preliminary took place in 2010 in Python, relying heavily on the NLTK and networkx packages.]

The following were extracted from my 2010 analysis for this paper:

\begin{verbatim}

    +----------+--------------+------+-----+---------+-------+
    | body     | mediumtext   | YES  |     | NULL    |       |
    | lastword | mediumtext   | YES  |     | NULL    |       |
    | hash     | varchar(250) | YES  | UNI | NULL    |       |
    | sender   | varchar(250) | YES  |     | NULL    |       |
    | tos      | text         | YES  |     | NULL    |       |
    | mid      | varchar(250) | YES  |     | NULL    |       |
    | ccs      | text         | YES  |     | NULL    |       |
    | date     | datetime     | YES  |     | NULL    |       |
    | subj     | varchar(500) | YES  |     | NULL    |       |
    | tosctn   | mediumint(9) | YES  |     | NULL    |       |
    | ccsctn   | mediumint(9) | YES  |     | NULL    |       |
    | source   | varchar(250) | YES  |     | NULL    |       |
    +----------+--------------+------+-----+---------+-------+
\end{verbatim}

The principal fields used in this paper are:

* sender
* date
* subject
* recipient
* new content (*lastword*)

### Conversion

Each email was a plaintext file^[Most had been generated by Microsoft Outlook, but some older emails were produced in IBM Notes, which created some character encoding issues.] Each user had a directory tree similar to the one below.^[This user had 10 directories with 3048 files (the directory tree has been pruned to omit spurious detail) containing 12,147 lines and 69,226 words.]


![Typical user data tree](img/dtree.jpg)


Although tedious, traversing the directory tree, parsing the emails and loading them into an SQL database, was accomplished with a combination of Python and Perl scripting and standard bash tools. I do not reproduce that process here as it has little bearing on the main topic of this paper.

### Data structure

While the emails were not in native format, the plain text versions contained nine principal segments, as shown in the figure below


![Structural analysis of an Enron email](img/parse_email.png)


### Deduplication

Using scripting tools, each text file extraction created a *payload* of the new content in the related email, capturing the text between the beginning metadata and the following metadata for email purposes. A `payload` hash, an [md5] encoded message digest^[In theory, it is possible that two non-identical sequences of bytes be encoded identically; the probability is low enough to make an md5 digest usable as a checksum verification, its purpose here.] was used in the initial analysis as a primary key to assure the uniqueness of each record. Approximately half of the corpus consisted of duplicates, such as the original message in the sender's sent file and one or more copies in the recipient's inbox, at a minimum. Multiple recipients and recipients who used email folders as a filing system were another source of duplicate messages. Applying this filter reduced the corpus to approximately 250,000 emails.

### Text isolation

For natural language processing (**NLP**) purposes, treating the `payload` rather than the `message body` as the unit of analysis avoided an *echo chamber* effect of `chains` quoting and re-quoting the original message, multiplying the frequency of the words it contained.

### Prioritization

Traditional analysis of emails was conducted on the principle that *something may be overlooked,* which delays the value of email in preliminary analysis. Prioritizing always leaves open the option of reviewing the set-asides later.

After deduplication, the first filter applied was to eliminate all email from external addresses that were not also recipients from internal addresses. Spam, newsletters and the like have low information potential. This filter reduced the remaining half of the corpus by half again, leaving approximately 125,000 emails.

A second filter for internal email was used to eliminate broadcast messages and high frequency administrative messages. Indicia of broadcast messages were large numbers of recipients, high frequency, paucity of return correspondence and keyword in context screening. Administrative messages to single recipients were identified by frequency, lack of return correspondence and high frequency words. Many of these were nagging emails concerning the lack of approval of expense reports, for example. This filter reduced the dataset to approximately 24,000.

The third filter limited the dataset to emails sent before Enron's December 2, 2001 bankruptcy. This filter reduced the email count to approximately 13,500, about 2.7% of the original total. A few emails dated "1979-12-31" were reviewed and deleted. The resulting dataset was named `g_enron` for its initial purpose, network graph analysis.

### Augmentation and transformation

Each unique Enron address in the reduced dataset was assigned a userid. The primary purpose was to facilitate social network analysis with node identifiers of uniform length; the second, to reduce analyst bias arising from gender stereotyping, frequency of exposure and similar subjective pattern seeking behaviors. 

To achieve a computationally practicable dataset for initial social network analysis, emails were limited to single Enron sender to Enron single recipient, reducing the dataset further, to 9,615 emails.

```{r, echo=FALSE, results="as-is", comment=NA, message=FALSE,  error = FALSE,  warning=FALSE}
plot_graph(Qall_sparse, "January 2000-December 2001")
```

The resulting graph had low density, and the the dataset was further trimmed by restricting it to reciprocal correspondents, each of whom sent an email to the other, either as a reply or an original message, excluding emails by a user to herself.

```{r, echo=FALSE, results="as-is", comment=NA, message=FALSE,  error = FALSE,  warning=FALSE}
plot_graph(Qall, "January 2000-December 2001")
```

## The network composition

The core Enron social network is defined to consist of those emails in the corpus consisting of one enron.com sender to one Enron.com recipient (excluding apparently automated accounts) if the recipient either replies to the sender or initiates a contact with the sender.

### Time frame

All emails from January 1, 2000 to December 2, 2001 the date of the [bankruptcy] were collected. A handful of messages prior to January 1, 2000 were excluded due to their small quarterly count.

### Participants

A total of `r comma2(nrow(userid))` unique users are represented. However, all but `r census(Qall)` users are non-reciprocating or isolated. To identify those, the sender and recipient userids were extracted and converted to a graph object, which will be referred to as the **core Enron corpus.** Following are the characteristics of the 23-month series. The attributes are 

* vertices: users
* directed: from-to and to-from distinguished
* hyper: contains emails from or to multiple users
* loops: includes email from user to herself
* multiple: multi-dimensional object
* bipartite: set of two vertices where no vertex in the same set is connected
* edges: number of emails

```{r, echo=FALSE, results="as-is", comment=NA, message=FALSE,  error = FALSE,  warning=FALSE}
Qall
```

Graph objects shown here represent users (vertices) by dots and emails (edges) by lines. The length of the line is not a measure of distance. The visualization algorithm arranges vertices and edges to promote recognition of connections only. It is also important to note that the visual representation of graph objects is not stable. The identical graph and identical algorithm will produce visual representations that portray the same graph object in different orientatinos. The relationships are preserved, but the geometery is not.


### Time series of core Enron corpus January 2000-December 2001

One outlier and several groups of outliers are apparent, mid-May 2001 and the weeks leading up to the [bankruptcy].

```{r, echo=FALSE, results="as-is", comment=NA, message=FALSE,  error = FALSE,  warning=FALSE}
plot_graph(Qall, "January 2000-December 2001")
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot_ts(Qall_ts, "January 2000-December 2001")
```

### Volume sent and received January 2000-December 2001

```{r, echo=FALSE, results="as-is", comment=NA, message=FALSE,  error = FALSE,  warning=FALSE}
send_p
```


```{r, echo=FALSE, results="as-is", comment=NA, message=FALSE,  error = FALSE,  warning=FALSE}
recd_p
```

### Quarterly breakdown of core Enron corpus

#### First quarter 2000

```{r, echo=FALSE, results="as-is", comment=NA, message=FALSE,  error = FALSE,  warning=FALSE}
Q100
plot_graph(Q100, "January-March 2000")
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot_ts(Q100_ts, "January-March 2000")
```

#### Second quarter 2000

```{r, echo=FALSE, results="as-is", comment=NA, message=FALSE,  error = FALSE,  warning=FALSE}
Q200
plot_graph(Q200, "April-June 2000")
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot_ts(Q200_ts, "April-June 2000")
```

#### Third quarter 2000

```{r, echo=FALSE, results="as-is", comment=NA, message=FALSE,  error = FALSE,  warning=FALSE}
Q300
plot_graph(Q300, "July-September 2000")
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot_ts(Q300_ts, "July-September 2000")
```

#### Fourth quarter 2000

```{r, echo=FALSE, results="as-is", comment=NA, message=FALSE,  error = FALSE,  warning=FALSE}
Q400
plot_graph(Q400, "October-December 2000")
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot_ts(Q400_ts, "October-December 2000")
```

#### First quarter 2001

```{r, echo=FALSE, results="as-is", comment=NA, message=FALSE,  error = FALSE,  warning=FALSE}
Q101
plot_graph(Q101, "January-March 2001")
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot_ts(Q101_ts, "January-March 2001")
```

#### Second quarter 2001

```{r, echo=FALSE, results="as-is", comment=NA, message=FALSE,  error = FALSE,  warning=FALSE}
Q101
plot_graph(Q201, "January-March 2001")
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot_ts(Q201_ts, "January-March 2001")
```


#### Third quarter 2001

```{r, echo=FALSE, results="as-is", comment=NA, message=FALSE,  error = FALSE,  warning=FALSE}
Q301
plot_graph(Q301, "July-October 2001")
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot_ts(Q301_ts, "July-October 2001")
```

#### Fourth quarter 2001

```{r, echo=FALSE, results="as-is", comment=NA, message=FALSE,  error = FALSE,  warning=FALSE}
Q401
plot_graph(Q401, "October-November 2000")
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
plot_ts(Q401_ts, "October-November 2001")
```

### Naive measures of user prominence

As illustrated by the quarterly tables and plot above, the number of users participating in the social network varied over time. However, only users `r All_qtrs` have activity in all seven quarters. In what other ways are these users distinguished?

```{r, echo=FALSE}
 ggnet2(Qall, label = All_qtrs, label.size = 4, label.color = "darkblue", node.size = 0.25, edge.size = 0.25, edge.color = "grey")
```

In the graph showing the three users active every quarter, two of them appear to be at the centers of groups and one, user 1718, appears on the periphery.

Alternatively, the top-10 list of users with reciprocal messages includes only user 1876.

```{r, echo=FALSE}

kable(top_n(msg_count,10)) %>%   kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
paired <- top_n(msg_count,10) %>% select(s_uid, r_uid) 
p_left <- paired[1]
p_right <- paired[2]
colnames(p_left) <- 'userid'
colnames(p_right) <- 'userid'
paired <- bind_rows(p_left, p_right) %>% distinct()
ggnet2(Qall, label = paired$userid, label.size = 4, label.color = "darkblue", node.size = 0.25, edge.size = 0.25, edge.color = "grey")
```

This view of user visibility presents a different impression. User 1876 is still the only user active over all four quarters who is included, and remains at the center of one of the patterns.

The top senders were

```{r, echo=FALSE}
kable(top_n(s_uid_count,25)) %>%  kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
to_show <- top_n(s_uid_count,25)%>% select(-n) %>% distinct() 
ggnet2(Qall, label = to_show$s_uid, label.size = 4, label.color = "darkblue", node.size = 0.25, edge.size = 0.25, edge.color = "grey")
```

The users 1876 and 1453 appear among the top senders.


The top recipients were

```{r, echo=FALSE}
kable(top_n(r_uid_count, 25)) %>%   kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
to_show <- top_n(r_uid_count,25)%>% select(-n) %>% distinct() 
ggnet2(Qall, label = to_show$r_uid, label.size = 4, label.color = "darkblue", node.size = 0.25, edge.size = 0.25, edge.color = "grey")
```

The users 1453 and 1876 are represented here, as well.

Only one user of the three most frequent email users was among the users in the top-pairs list, the top senders list and the top receivers list.

### Graph measures of user connectedness

#### Degree

`Degree` is one of many metrics that measure the relative position in a graph of a vertex, or user. It is simply the number of connections with different users. The top 10 users ranked by that criterion are:

```{r, echo=FALSE}
kable(deg(Qall,10)) %>%   kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
ggnet2(Qall, label = deg(Qall,10)$userid, label.size = 4, label.color = "darkblue", node.size = 0.25, edge.size = 0.25, edge.color = "grey")
```

This metric introduces a new group of users unseen in the naive analysis.

### Closeness

`Closeness` measures the geodesic bewtween a vertex and all other vertices in a graph.

```{r, echo=FALSE}
kable(clos(Qall,25)) %>%   kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
ggnet2(Qall, label = clos(Qall,10)$userid, label.size = 4, label.color = "darkblue", node.size = 0.25, edge.size = 0.25, edge.color = "grey")
```

### Betweeness

`Betweenness` is a measure of the extent to which graph paths use a vertex to connect other vertices.


```{r, echo=FALSE}
kable(betw(Qall,25)) %>%   kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
ggnet2(Qall, label = betw(Qall,10)$userid, label.size = 4, label.color = "darkblue", node.size = 0.25, edge.size = 0.25, edge.color = "grey")
```

### Usefullness

Which of these measures to privilege is not clear. They each present different perspectives of the relative importance of the user in the network, based on different criteria, but none fails to identify any collection of users to which attention should be directed.

[staff report]:http://elibrary.ferc.gov/idmws/common/opennat.asp?fileID=9548231

[Wolak report]:http://www.caiso.com/Documents/ReportonRedesign-CaliforniaReal-TimeEnergyandAncillaryServicesMarkets.pdf

[Anatole Kalesky]:https://www.project-syndicate.org/commentary/canceling-brexit-becoming-inevitable-by-anatole-kaletsky-2018-12

[Enron Corp.]:https://en.wikipedia.org/wiki/Enron

[Annual Report on Form 10-K for the year ended December 31, 2000]:https://www.sec.gov/Archives/edgar/data/1024401/000102440101500010/ene10-k.txt

[bankruptcy protection]:(https://www.sec.gov/Archives/edgar/data/1024401/000102440101500046/ene8-k1214.txt

[bankruptcy]:(https://www.sec.gov/Archives/edgar/data/1024401/000102440101500046/ene8-k1214.txt

[became aware]:https://www.ferc.gov/industries/electric/indus-act/wec/chron/chronology.pdf

[testimony]:https://web.stanford.edu/group/fwolak/cgi-bin/sites/default/files/files/2002,%20May%2015_Senate%20Committee%20on%20Commerce,%20Science%20and%20Transportation_Wolak.pdf

[Elephant and the Blind Men Problem]: https://buddhismnow.com/2018/02/16/tittha-sutta-buddhist-parable-of-the-blind-men-and-the-elephant/

[md5]:https://en.wikipedia.org/wiki/MD5

[latentnet]:https://CRAN.R-project.org/package=latentnet

[published]:https://www.cs.cmu.edu/~./enron/

[redacted]:https://www.cs.cmu.edu/~./enron/DELETIONS.txt

[2009 version]:https://www.cs.cmu.edu/~./enron/

[six degrees of Kevin Bacon]:https://oracleofbacon.org/

[Keijo Ruohonen]:http://math.tut.fi/~ruohonen/GT_English.pdf

[letentnet paper]:https://www.jstatsoft.org/article/view/v024i05

[theoretical]:https://www.jstatsoft.org/article/view/v024i05

[accessible]:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2831261/

[The Federal Rules of Civil Procedure]:https://www.uscourts.gov/sites/default/files/cv_rules_eff._dec._1_2018_0.pdf

[McWhorter]:https://www.ted.com/talks/john_mcwhorter_txtng_is_killing_language_jk?language=en

[Sedona Conference]:https://thesedonaconference.org/download-publication?fid=2978

[ESI Guidelines]:https://www.cand.uscourts.gov/eDiscoveryGuidelines

[Advisory Committee]:https://www.uscourts.gov/rules-policies/archives/committee-reports/advisory-committee-rules-civil-procedure-may-2014

[ESI Checklist]:https://www.cand.uscourts.gov/eDiscoveryGuidelines

[keyword limitations]:http://www.neworleansbar.org/news/committees/relying-on-keyword-search-for-e-discovery-it-may-harm-your-case

[classic paper]:http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf

[Tidytext]:https://www.tidytextmining.com/






